{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #3: A simple language classifier\n",
    "\n",
    "#### Author: Hicham Mohamad (hi8826mo-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Scope of the lab](#t1)\n",
    "2. [Programming: Extracting the features](#t2)\n",
    "3. [Programming: Building $\\mathbf{X}$](#t3)\n",
    "4. [Programming: Building $\\mathbf{y}$](#t4)\n",
    "5. [Programming: Building the Model](#t5)\n",
    "6. [Predicting](#t6)\n",
    "7. [Predict the language of a text](#t7)\n",
    "8. [Submission](#t8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a **language detector** inspired from Google's _Compact language detector_, version 3 (CLD3): https://github.com/google/cld3. **CLD3** is written in C++ and its code is available from GitHub. The objectives of the assignment are to:\n",
    "* Write a program to classify languages\n",
    "* Use neural networks\n",
    "* Know what a classifier is\n",
    "* Write a short report of 1 to 2 pages to describe your program. You will notably comment the performance you obtained and how you could improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the GitHub description of CLD3, https://github.com/google/cld3, (_Model_ section). In your individual report you will:\n",
    "1. Summarize the system in two or three sentences;\n",
    "2. Outline the CLD3 overall architecture in a figure. Use building blocks only and do not specify the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As dataset, we will use **Tatoeba**, https://tatoeba.org/eng/downloads. It consists of more than 8 million short texts in 347 languages and it is available in one file called `sentences.csv`.\n",
    "\n",
    "The dataset is structured in this way: There is one text per line, where each line consists of the three following fields separated by tabulations and ended by a carriage return:\n",
    "```\n",
    "sentence id [tab] language code [tab] text [cr]\n",
    "```\n",
    "Each text (sentence) has a unique id and has a language code that follows the ISO 639-3 standard (see below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: \n",
    "\n",
    "CLD3 is a **neural network model** for language identification. The code package on Github contains the inference code and a trained model. The inference code extracts character **ngrams** from the input text and computes the **fraction of times** each of them appears. For example, as shown in the figure below, if the input text is \"banana\", then one of the extracted trigrams is \"ana\" and the corresponding fraction is 2/4. The ngrams are hashed down to an id within a small range, and each id is represented by a dense embedding vector estimated during training.\n",
    "\n",
    "The model averages the embeddings corresponding to each ngram type according to the fractions, and the averaged embeddings are concatenated to produce the embedding layer. The remaining components of the network are a hidden (Rectified linear) layer and a softmax layer.\n",
    "\n",
    "To get a language prediction for the input text, we simply perform a forward pass through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope of the lab <a name=\"t1\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will consider three languages only: French (fra), English (eng), and Swedish (swe). Below is an excerpt of the **Tatoeba dataset** limited to these three languages: \n",
    "\n",
    "```\n",
    "1276    eng     Let's try something.\n",
    "1277    eng     I have to go to sleep.\n",
    "1280    eng     Today is June 18th and it is Muiriel's birthday!\n",
    "...\n",
    "1115    fra     Lorsqu'il a demandé qui avait cassé la fenêtre, tous les garçons ont pris un air innocent.\n",
    "1279    fra     Je ne supporte pas ce type.\n",
    "1441    fra     Pour une fois dans ma vie je fais un bon geste... Et ça ne sert à rien.\n",
    "...\n",
    "337413  swe     Vi trodde att det var ett flygande tefat.\n",
    "341910  swe     Detta är huset jag bodde i när jag var barn.\n",
    "341938  swe     Vi hade roligt på stranden igår.\n",
    "...\n",
    "```\n",
    "Tatoeba is updated continuously. The examples from this dataset come from a corpus your instructor downloaded on September 15, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the $\\mathbf{X}$ matrix (feature matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now investigate the CLD3 features:\n",
    " *  What are the **features** CLD3 extracts from each text?\n",
    " * Create manually a simplified $\\mathbf{X}$ matrix where you will **represent the 9 texts with CLD3 features**. You will use a restricted set of features: You will only consider the letters _a_, _b_, and _n_ and the bigrams _an_, _ba_, and _na_. You will ignore the the rest of letters and bigrams as well as the trigrams. Your matrix will have 9 rows and 6 columns, each column will contain these counts: `[#a, #b, #n, #an, #ba, #na]`.\n",
    "\n",
    "The CLD3's original description uses **relative frequencies** (counts of a letter divided by the total counts of letters in the text). Here, you will use the **raw counts**. To help you, your instructor filled the fourth row of the matrix corresponding to the first text in French. Fill in the rest. You will include this matrix in your report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "0& 0& 1& 0& 0& 0\\\\\n",
    "0& 0& 0& 0& 0& 0\\\\\n",
    "3& 1& 2& 1& 0& 0\\\\\n",
    "8& 0& 8& 1& 0& 0\\\\\n",
    "1& 0& 1& 0& 0& 0\\\\\n",
    "4& 1& 6& 0& 0& 0\\\\\n",
    "4& 0& 1& 1& 0& 0\\\\\n",
    "5& 2& 2& 0& 1& 0\\\\\n",
    "2& 0& 1& 1& 0& 0\\\\\n",
    "\\end{bmatrix}$\n",
    "; $\\mathbf{y} =\n",
    "\\begin{bmatrix}\n",
    "     \\text{eng} \\\\\n",
    "     \\text{eng}\\\\\n",
    "     \\text{eng}\\\\\n",
    "    \\text{fra}\\\\\n",
    "   \\text{fra}  \\\\\n",
    "     \\text{fra}\\\\\n",
    "    \\text{swe}\\\\\n",
    " \\text{swe}   \\\\\n",
    " \\text{swe}   \n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming: Extracting the features <a name=\"t2\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start programming, download the Tatoeba dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and filtering the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code to read the dataset and split it into lines. You may have to change the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\tcmn\\t我們試試看！',\n",
       " '2\\tcmn\\t我该去睡觉了。',\n",
       " '3\\tcmn\\t你在干什麼啊？',\n",
       " '4\\tcmn\\t這是什麼啊？',\n",
       " '5\\tcmn\\t今天是６月１８号，也是Muiriel的生日！',\n",
       " '6\\tcmn\\t生日快乐，Muiriel！',\n",
       " '7\\tcmn\\tMuiriel现在20岁了。',\n",
       " '8\\tcmn\\t密码是\"Muiriel\"。',\n",
       " '9\\tcmn\\t我很快就會回來。',\n",
       " '10\\tcmn\\t我不知道。']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = open('../../corpus/sentences.csv', encoding='utf8').read().strip()\n",
    "dataset = open('sentences/sentences.csv', encoding='utf8').read().strip()\n",
    "dataset = dataset.split('\\n')\n",
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code to **split the fields** and remove possible **whitespaces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'cmn', '我們試試看！'), ('2', 'cmn', '我该去睡觉了。'), ('3', 'cmn', '你在干什麼啊？')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = list(map(lambda x: tuple(x.split('\\t')), dataset))\n",
    "#dataset[:3]\n",
    "dataset = list(map(lambda x: tuple(map(str.strip, x)), dataset))\n",
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cmn'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write the code to extract the **French, English, and Swedish** texts. You will call the resulting dataset: `dataset_small`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "#for sentence in dataset:\n",
    "    #if (sentence[1] is ['eng', 'fra', 'swe']):\n",
    "#    if (sentence[1] == 'swe'):\n",
    "#        print(sentence)\n",
    "#        dataset_small = list(filter(lambda sentence: sentence[1]=='swe', dataset))\n",
    "dataset_small = list(filter(lambda sentence: sentence[1] in ['eng','fra','swe'], dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1115',\n",
       "  'fra',\n",
       "  \"Lorsqu'il a demandé qui avait cassé la fenêtre, tous les garçons ont pris un air innocent.\"),\n",
       " ('1276', 'eng', \"Let's try something.\"),\n",
       " ('1277', 'eng', 'I have to go to sleep.'),\n",
       " ('1279', 'fra', 'Je ne supporte pas ce type.'),\n",
       " ('1280', 'eng', \"Today is June 18th and it is Muiriel's birthday!\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_small[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Count Characters Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function `count_chars(string, lc=True)` to count characters (**unigrams**) of a string. You will set the text in **lowercase** if `lc` is set to `True`. As in CLD3, you will return the **relative frequencies** of the unigrams, i.e. counts of a letter divided by the total counts of letters in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def count_chars(string, lc=True):\n",
    "    frequency = {}\n",
    "    if lc != False:\n",
    "        string = string.lower()\n",
    "    #print(len(string))\n",
    "    #words = string.split()\n",
    "    #for word in words:\n",
    "    for c in range(len(string)):\n",
    "        if string[c] in frequency:\n",
    "            frequency[string[c]] += 1\n",
    "        else:\n",
    "            frequency[string[c]] = 1\n",
    "            \n",
    "        #print(list(word))\n",
    "        #unigrams = list(map(lambda c: c, word))\n",
    "        #print(unigrams)\n",
    "#    for char in len(string):\n",
    "#        chars = string[char].split()\n",
    "    #return list(map(lambda f: f/len(string), list(frequency.values())))\n",
    "    #map(lambda f: f/len(string), list(frequency.items()))\n",
    "    #map(lambda f: f/len(string), frequency.items()[1])\n",
    "    \n",
    "    # dict comprehensions can be used to create dictionaries \n",
    "    # from arbitrary key and value expressions\n",
    "    frequency = {k:v/len(string) for k, v in frequency.items()}\n",
    "    \n",
    "    #print(list(frequency.values())/len(string))\n",
    "    #print(frequency.keys())\n",
    "    #print(frequency)\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_chars2(string, lc=True):     \n",
    "    counts = {}     \n",
    "    if lc:         \n",
    "        string = string.lower()     \n",
    "        for char in string:         \n",
    "            if char in counts:             \n",
    "                counts[char] += 1         \n",
    "            else:             \n",
    "                counts[char] = 1\n",
    "                \n",
    "    sum_chars = sum(counts.values()) \n",
    "    counts = {k:v/sum_chars for k, v in counts.items()}     \n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function `count_bigrams(string, lc=True)` to count the characters **bigrams** of a string. You will set the text in lowercase if `lc` is set to `True`. As in CLD3, you will return the relative frequencies of the bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def count_bigrams(string, lc=True):\n",
    "    frequency = {}\n",
    "    if lc!=False:\n",
    "        string = string.lower()\n",
    "        \n",
    "    bigrams = []\n",
    "    for i in range(len(string) - 1):\n",
    "        bigrams.append(string[i:i+2])\n",
    "    #print(len(string))\n",
    "    #words = string.split()\n",
    "    #for word in words:\n",
    "    for bi in range(len(bigrams)):\n",
    "        if bigrams[bi] in frequency:\n",
    "            frequency[bigrams[bi]] += 1\n",
    "            #frequency[bigrams[bi]] = frequency[bigrams[bi]]/len(string)\n",
    "        else:\n",
    "            frequency[bigrams[bi]] = 1\n",
    "            #frequency[string[c]] /= len(string)\n",
    "            #frequency[bigrams[bi]] = frequency[bigrams[bi]]/len(string)\n",
    "        #print(list(word))\n",
    "        #unigrams = list(map(lambda c: c, word))\n",
    "        #print(unigrams)\n",
    "#    for char in len(string):\n",
    "#        chars = string[char].split()\n",
    "\n",
    "    # dict comprehensions can be used to create dictionaries \n",
    "    # from arbitrary key and value expressions\n",
    "    frequency = {k:v/len(string) for k, v in frequency.items()}\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `count_trigrams(string, lc=True)` to count the characters **trigrams** of a string. You will set the text in lowercase if `lc` is set to `True`. As in CLD3, you will return the relative frequencies of the trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def count_trigrams(string, lc=True):\n",
    "    frequency = {}\n",
    "    if lc!=False:\n",
    "        string = string.lower()\n",
    "        \n",
    "    trigrams = []\n",
    "    for i in range(len(string) - 3 + 1):\n",
    "        trigrams.append(string[i:i+3])\n",
    "    #print(len(string))\n",
    "    #words = string.split()\n",
    "    #for word in words:\n",
    "    for tri in range(len(trigrams)):\n",
    "        if trigrams[tri] in frequency:\n",
    "            frequency[trigrams[tri]] += 1\n",
    "        else:\n",
    "            frequency[trigrams[tri]] = 1\n",
    "            \n",
    "        #print(list(word))\n",
    "        #unigrams = list(map(lambda c: c, word))\n",
    "        #print(unigrams)\n",
    "#    for char in len(string):\n",
    "#        chars = string[char].split()\n",
    "\n",
    "    # dict comprehensions can be used to create dictionaries \n",
    "    # from arbitrary key and value expressions\n",
    "    frequency = {k:v/len(string) for k, v in frequency.items()}\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l': 0.05,\n",
       " 'e': 0.1,\n",
       " 't': 0.15,\n",
       " \"'\": 0.05,\n",
       " 's': 0.1,\n",
       " ' ': 0.1,\n",
       " 'r': 0.05,\n",
       " 'y': 0.05,\n",
       " 'o': 0.05,\n",
       " 'm': 0.05,\n",
       " 'h': 0.05,\n",
       " 'i': 0.05,\n",
       " 'n': 0.05,\n",
       " 'g': 0.05,\n",
       " '.': 0.05}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_chars(\"Let's try something.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'le': 0.05,\n",
       " 'et': 0.1,\n",
       " \"t'\": 0.05,\n",
       " \"'s\": 0.05,\n",
       " 's ': 0.05,\n",
       " ' t': 0.05,\n",
       " 'tr': 0.05,\n",
       " 'ry': 0.05,\n",
       " 'y ': 0.05,\n",
       " ' s': 0.05,\n",
       " 'so': 0.05,\n",
       " 'om': 0.05,\n",
       " 'me': 0.05,\n",
       " 'th': 0.05,\n",
       " 'hi': 0.05,\n",
       " 'in': 0.05,\n",
       " 'ng': 0.05,\n",
       " 'g.': 0.05}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_bigrams(\"Let's try something.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'let': 0.05,\n",
       " \"et'\": 0.05,\n",
       " \"t's\": 0.05,\n",
       " \"'s \": 0.05,\n",
       " 's t': 0.05,\n",
       " ' tr': 0.05,\n",
       " 'try': 0.05,\n",
       " 'ry ': 0.05,\n",
       " 'y s': 0.05,\n",
       " ' so': 0.05,\n",
       " 'som': 0.05,\n",
       " 'ome': 0.05,\n",
       " 'met': 0.05,\n",
       " 'eth': 0.05,\n",
       " 'thi': 0.05,\n",
       " 'hin': 0.05,\n",
       " 'ing': 0.05,\n",
       " 'ng.': 0.05}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_trigrams(\"Let's try something.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the ngrams in the dataset (modified: only unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now **extract the features** from each text. For this, add the character, bigram, and trigram **relative frequencies** to the texts using this format:\n",
    "`(text_id, language_id, text, char_cnt, bigram_cnt, trigram_cnt)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the datapoint:\n",
    "`('1276', 'eng', \"Let's try something.\")`,\n",
    "you must return:\n",
    "\n",
    "`('1276', 'eng', \"Let's try something.\", \n",
    "  {'l': 0.05, 'e': 0.1, 't': 0.15, \"'\": 0.05, 's': 0.1, ' ': 0.1, 'r': 0.05, 'y': 0.05, 'o': 0.05, 'm': 0.05, 'h': 0.05, 'i': 0.05, 'n': 0.05, 'g': 0.05, '.': 0.05},\n",
    "  {'le': 0.05263157894736842, 'et': 0.10526315789473684, \"t'\": 0.05263157894736842, \"'s\": 0.05263157894736842, 's ': 0.05263157894736842, ' t': 0.05263157894736842, 'tr': 0.05263157894736842, 'ry': 0.05263157894736842, 'y ': 0.05263157894736842, ' s': 0.05263157894736842, 'so': 0.05263157894736842, 'om': 0.05263157894736842, 'me': 0.05263157894736842, 'th': 0.05263157894736842, 'hi': 0.05263157894736842, 'in': 0.05263157894736842, 'ng': 0.05263157894736842, 'g.': 0.05263157894736842},\n",
    "  {'let': 0.05555555555555555, \"et'\": 0.05555555555555555, \"t's\": 0.05555555555555555, \"'s \": 0.05555555555555555, 's t': 0.05555555555555555, ' tr': 0.05555555555555555, 'try': 0.05555555555555555, 'ry ': 0.05555555555555555, 'y s': 0.05555555555555555, ' so': 0.05555555555555555, 'som': 0.05555555555555555, 'ome': 0.05555555555555555, 'met': 0.05555555555555555, 'eth': 0.05555555555555555, 'thi': 0.05555555555555555, 'hin': 0.05555555555555555, 'ing': 0.05555555555555555, 'ng.': 0.05555555555555555})`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You will store the **extracted features** in a **list** that you will call `dataset_small_feat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: (MODIFIED TASK) \n",
    "You will now extract the features from each text. For this, add ONLY the character , AND NOT bigram and trigram, relative frequencies to the texts using this format: (text_id, language_id, text, char_cnt), i.e. without bigram_cnt, trigram_cnt !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# we can only concatenate tuple (not \"dict\") to tuple\n",
    "dataset_small_feat = list(map(lambda sentence: sentence + (count_chars(sentence[2]), ), \n",
    "                              dataset_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1115',\n",
       "  'fra',\n",
       "  \"Lorsqu'il a demandé qui avait cassé la fenêtre, tous les garçons ont pris un air innocent.\",\n",
       "  {'l': 0.044444444444444446,\n",
       "   'o': 0.05555555555555555,\n",
       "   'r': 0.05555555555555555,\n",
       "   's': 0.07777777777777778,\n",
       "   'q': 0.022222222222222223,\n",
       "   'u': 0.044444444444444446,\n",
       "   \"'\": 0.011111111111111112,\n",
       "   'i': 0.06666666666666667,\n",
       "   ' ': 0.16666666666666666,\n",
       "   'a': 0.08888888888888889,\n",
       "   'd': 0.022222222222222223,\n",
       "   'e': 0.05555555555555555,\n",
       "   'm': 0.011111111111111112,\n",
       "   'n': 0.08888888888888889,\n",
       "   'é': 0.022222222222222223,\n",
       "   'v': 0.011111111111111112,\n",
       "   't': 0.05555555555555555,\n",
       "   'c': 0.022222222222222223,\n",
       "   'f': 0.011111111111111112,\n",
       "   'ê': 0.011111111111111112,\n",
       "   ',': 0.011111111111111112,\n",
       "   'g': 0.011111111111111112,\n",
       "   'ç': 0.011111111111111112,\n",
       "   'p': 0.011111111111111112,\n",
       "   '.': 0.011111111111111112}),\n",
       " ('1276',\n",
       "  'eng',\n",
       "  \"Let's try something.\",\n",
       "  {'l': 0.05,\n",
       "   'e': 0.1,\n",
       "   't': 0.15,\n",
       "   \"'\": 0.05,\n",
       "   's': 0.1,\n",
       "   ' ': 0.1,\n",
       "   'r': 0.05,\n",
       "   'y': 0.05,\n",
       "   'o': 0.05,\n",
       "   'm': 0.05,\n",
       "   'h': 0.05,\n",
       "   'i': 0.05,\n",
       "   'n': 0.05,\n",
       "   'g': 0.05,\n",
       "   '.': 0.05})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_small_feat[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unigram frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('l', 0.044444444444444446), ('o', 0.05555555555555555), ('r', 0.05555555555555555), ('s', 0.07777777777777778), ('q', 0.022222222222222223), ('u', 0.044444444444444446), (\"'\", 0.011111111111111112), ('i', 0.06666666666666667), (' ', 0.16666666666666666), ('a', 0.08888888888888889), ('d', 0.022222222222222223), ('e', 0.05555555555555555), ('m', 0.011111111111111112), ('n', 0.08888888888888889), ('é', 0.022222222222222223), ('v', 0.011111111111111112), ('t', 0.05555555555555555), ('c', 0.022222222222222223), ('f', 0.011111111111111112), ('ê', 0.011111111111111112), (',', 0.011111111111111112), ('g', 0.011111111111111112), ('ç', 0.011111111111111112), ('p', 0.011111111111111112), ('.', 0.011111111111111112)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset_small is of this format: (text_id, language_id, text, char_cnt)\n",
    "dataset_small_feat[0][3].items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigram frequencies (NOTE: this is cancelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_small_feat[0][4].items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming: Building $\\mathbf{X}$ <a name=\"t3\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now build the $\\mathbf{X}$ matrix. In this assignment, you will only consider **unigrams** to speed up the training step. **This means that you will set aside the character bigrams and trigrams**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done with the lab requirements, feel free to improve the program and include bigrams and trigrams. To add bigrams, a possible method is to add the bigram dictionary to the unigram one using **update** and then to extract the resulting dictionary. You can easily extend this to trigrams. Feel free to use another method if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_BIGRAMS = False\n",
    "if INCLUDE_BIGRAMS:\n",
    "    for i in range(len(dataset_small_feat)):\n",
    "        dataset_small_feat[i][3].update(dataset_small_feat[i][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CLD3 architecture uses **embeddings**. In this lab, we will simplify it and we will use a **feature vector** instead consisting of the **character frequencies**. For example, you will represent the text:\n",
    "\n",
    "`\"Let's try something.\"`\n",
    "\n",
    "with:\n",
    "\n",
    "`{'l': 0.05, 'e': 0.1, 't': 0.15, \"'\": 0.05, 's': 0.1, ' ': 0.1, \n",
    " 'r': 0.05, 'y': 0.05, 'o': 0.05, 'm': 0.05, 'h': 0.05, 'i': 0.05, \n",
    " 'n': 0.05, 'g': 0.05, '.': 0.05}`\n",
    "\n",
    "To create the $\\mathbf{X}$ matrix, we need to transform the dictionaries of `dataset_small` into **numerical vectors**. The `DictVectorizer` class from the **scikit-learn** library, see here [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html], has two methods, `fit()` and `transform()`, and a combination of both `fit_transform()` to convert dictionaries into such vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now write the code to:\n",
    "\n",
    "1. Extract the character frequency dictionaries from `dataset_small` corresponding to its 3rd index and set them in a list;\n",
    "2. Convert the list of dictionaries into an $\\mathbf{X}$ matrix using `DictVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the character frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a new list of datapoints with the **unigrams** only. Each item in this list will be a dictionary. You will call it `X_cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "X_cat = list(map(lambda sentence: count_chars(sentence[2]), dataset_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'l': 0.044444444444444446,\n",
       "  'o': 0.05555555555555555,\n",
       "  'r': 0.05555555555555555,\n",
       "  's': 0.07777777777777778,\n",
       "  'q': 0.022222222222222223,\n",
       "  'u': 0.044444444444444446,\n",
       "  \"'\": 0.011111111111111112,\n",
       "  'i': 0.06666666666666667,\n",
       "  ' ': 0.16666666666666666,\n",
       "  'a': 0.08888888888888889,\n",
       "  'd': 0.022222222222222223,\n",
       "  'e': 0.05555555555555555,\n",
       "  'm': 0.011111111111111112,\n",
       "  'n': 0.08888888888888889,\n",
       "  'é': 0.022222222222222223,\n",
       "  'v': 0.011111111111111112,\n",
       "  't': 0.05555555555555555,\n",
       "  'c': 0.022222222222222223,\n",
       "  'f': 0.011111111111111112,\n",
       "  'ê': 0.011111111111111112,\n",
       "  ',': 0.011111111111111112,\n",
       "  'g': 0.011111111111111112,\n",
       "  'ç': 0.011111111111111112,\n",
       "  'p': 0.011111111111111112,\n",
       "  '.': 0.011111111111111112},\n",
       " {'l': 0.05,\n",
       "  'e': 0.1,\n",
       "  't': 0.15,\n",
       "  \"'\": 0.05,\n",
       "  's': 0.1,\n",
       "  ' ': 0.1,\n",
       "  'r': 0.05,\n",
       "  'y': 0.05,\n",
       "  'o': 0.05,\n",
       "  'm': 0.05,\n",
       "  'h': 0.05,\n",
       "  'i': 0.05,\n",
       "  'n': 0.05,\n",
       "  'g': 0.05,\n",
       "  '.': 0.05}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cat[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize `X_cat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert your `X_cat` matrix into a **numerical representation** using `DictVectorizer`: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The class DictVectorizer can be used to convert feature arrays represented as lists of standard Python dict objects to the NumPy/SciPy representation used by scikit-learn estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# Encoding the features\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming: Building $\\mathbf{y}$ <a name=\"t4\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now convert the list of **language symbols** into a $\\mathbf{y}$ vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the **language symbols** from `dataset_small_feat` and call the resulting list `y_cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "y_cat = list(map(lambda sentence: sentence[1], dataset_small_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fra', 'eng', 'eng', 'fra', 'eng']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build two indices mapping the symbols to integers and the integers to symbols. Both indices will be **dictionaries** that you will call: `lang2inx`and `inx2lang`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "y_symbols = ['fra', 'eng', 'swe']\n",
    "inx2lang = dict(enumerate(y_symbols))\n",
    "\n",
    "# we build an inverted dictionary\n",
    "lang2inx = {v:k for k, v in inx2lang.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'fra', 1: 'eng', 2: 'swe'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inx2lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fra': 0, 'eng': 1, 'swe': 2}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang2inx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert your `y_cat` vector into a numerical vector. Call this vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "y = list(map(lambda lang: lang2inx[lang], y_cat))\n",
    "#y = [lang2inx[i] for i in y_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming: Building the Model <a name=\"t5\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a neural network using sklearn with a **hidden layer** of 50 nodes and a **relu activation** layer: https://scikit-learn.org/stable/modules/neural_networks_supervised.html. Set the maximal number of **iterations** to 5, in the beginning, and **verbose** to True. Use the default values for the rest. You will call you classifier `clf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "#classifier = linear_model.LogisticRegression(penalty='l2', dual=True, solver='liblinear')\n",
    "clf = MLPClassifier(activation='relu', hidden_layer_sizes=(50,), \n",
    "                    verbose=True, max_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(50,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=5, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now split the dataset into training and a validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We shuffle the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[514315, 1749829, 740705, 1459710, 739350, 1535081, 1839597, 1581195, 441083, 792085]\n",
      "(1844225, 354)\n",
      "(1844225,)\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(X.shape[0]))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "print(indices[:10])\n",
    "\n",
    "X = X[indices, :]\n",
    "y = np.array(y)[indices]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:training_examples, :]\n",
    "y_train = y[:training_examples]\n",
    "\n",
    "X_val = X[training_examples:, :]\n",
    "y_val = y[training_examples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: MLPClassifier\n",
    "\n",
    "MLPClassifier, Multi-layer Perceptron classifier, trains iteratively since at each time step the partial derivatives of the **loss function** with respect to the model parameters are computed to update the parameters.\n",
    "\n",
    "It can also have a **regularization** term added to the loss function that shrinks model parameters to prevent **overfitting**.\n",
    "\n",
    "This implementation works with data represented as dense numpy arrays or sparse scipy arrays of floating point values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.12518509\n",
      "Iteration 2, loss = 0.07273527\n",
      "Iteration 3, loss = 0.06674220\n",
      "Iteration 4, loss = 0.06253729\n",
      "Iteration 5, loss = 0.05981266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hashu\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(50,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=5, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting <a name=\"t6\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the `X_val` languages. You will call the result `y_val_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "y_val_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 1 2 0 1 1 0]\n",
      "[1 0 0 1 1 2 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_val_pred[:10])\n",
    "print(y_val[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9788203717008499"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "accuracy_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        fra       0.97      0.95      0.96     87715\n",
      "        eng       0.98      0.99      0.99    273517\n",
      "        swe       0.97      0.89      0.93      7613\n",
      "\n",
      "avg / total       0.98      0.98      0.98    368845\n",
      "\n",
      "Micro F1: 0.9788203717008499\n",
      "Macro F1 0.9583552353995707\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_val_pred, target_names=y_symbols))\n",
    "print('Micro F1:', f1_score(y_val, y_val_pred, average='micro'))\n",
    "print('Macro F1', f1_score(y_val, y_val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 83759,   3915,     41],\n",
       "       [  2852, 270482,    183],\n",
       "       [    77,    744,   6792]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Increase the number of iterations to improve the **score**. You may also change the **parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the language of a text <a name=\"t7\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now predict the languages of the strings below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"Salut les gars !\", \"Hejsan grabbar!\", \"Hello guys!\", \"Hejsan tjejer!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create **features vectors** from this list. Call this matrix `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# Extract the character frequency dictionaries from the strings list 'docs'\n",
    "X_docs = list(map(lambda sentence: count_chars(sentence), docs))\n",
    "#X_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 354)\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of dictionaries into an X matrix using DictVectorizer\n",
    "# Vectorize X_freq (Encoding the features)\n",
    "#vec = DictVectorizer(sparse=True)\n",
    "#X_docs = vec.fit(X_docs)\n",
    "#X_test = vec.fit_transform(X_docs)\n",
    "X_test = vec.transform(X_docs)\n",
    "print(X_test.shape)\n",
    "\n",
    "#y_docs = list(map(lambda sentence: sentence, dataset_small_feat))\n",
    "# building y\n",
    "#y_test = list(map(lambda lang: lang2inx[lang], y_docs))\n",
    "#y = [lang2inx[i] for i in y_cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And run the **prediction** that you will store in a variable called `pred_languages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "pred_languages = clf.predict(X_test)\n",
    "pred_languages = [inx2lang.get(i) for i in pred_languages ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fra', 'swe', 'eng', 'swe']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission <a name=\"t8\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have written all the code and run all the cells, fill in your ID and as well as the name of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "STIL_ID = [\"hi8826mo-s\"] # Write your stil ids as a list\n",
    "CURRENT_NOTEBOOK_PATH = os.path.join(os.getcwd(), \n",
    "                                     \"3_language_detector_HichamMohamad.ipynb\") # Write the name of your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission code will send your answer. It consists of the predicted languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"pred_langs\": [\"fra\", \"swe\", \"eng\", \"swe\"]}'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANSWER = json.dumps({'pred_langs': pred_languages})\n",
    "ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the moment of truth:\n",
    "1. Save your notebook and\n",
    "2. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_NOTEBOOK_PATH = CURRENT_NOTEBOOK_PATH + \".submission.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSIGNMENT = 3\n",
    "API_KEY = \"f581ba347babfea0b8f2c74a3a6776a7\"\n",
    "\n",
    "# Copy and compress current notebook\n",
    "with bz2.open(SUBMISSION_NOTEBOOK_PATH, mode=\"wb\") as fout:\n",
    "    with open(CURRENT_NOTEBOOK_PATH, \"rb\") as fin:\n",
    "        fout.write(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'msg': None,\n",
       " 'status': 'correct',\n",
       " 'signature': '0fc40478b302739bcea019cabc596cf85f88c764f3cc49d0bba05f2efef9940f62629668fc1a0ad4d0727ad1bf9e322fbd44f839df6408fada9fd6a398daf46f',\n",
       " 'submission_id': 'a91e248b-de96-44a4-a49e-7465878e8337'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.post(\"https://vilde.cs.lth.se/edan20checker/submit\", \n",
    "                    files={\"notebook_file\": open(SUBMISSION_NOTEBOOK_PATH, \"rb\")}, \n",
    "                    data={\n",
    "                        \"stil_id\": STIL_ID,\n",
    "                        \"assignment\": ASSIGNMENT,\n",
    "                        \"answer\": ANSWER,\n",
    "                        \"api_key\": API_KEY,\n",
    "                    },\n",
    "               verify=True)\n",
    "\n",
    "# from IPython.display import display, JSON\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postscript from Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created this assignment from an examination I wrote last year for the course on applied machine learning. I simplified it from the `README.md` on GitHub, https://github.com/google/cld3. I found the C++ code difficult to understand and I reimplemented a Keras/Tensorflow version of it from this `README`. Should you be interested, you can find it here: https://github.com/pnugues/language-detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
