{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #4: Extracting syntactic groups using machine-learning techniques\n",
    "\n",
    "#### Author: Hicham Mohamad (hi8826mo-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Loading the corpus](#t1)\n",
    "2. [Baseline chunker](#t2)\n",
    "3. [Using Machine Learning: A first ML program](#t3)\n",
    "4. [Using Machine Learning: Adding all the features from Kudoh and Matsumoto](#t4)\n",
    "5. [Collecting the entities](#t5)\n",
    "6. [Resolving the entities](#t6)\n",
    "7. [Submission](#t7)\n",
    "8. [Reading](#t8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will create a system to extract **syntactic groups** from a text. You will apply it to the **CoNLL 2000** dataset. In addition, you will try to link a few extracted **named entities** to real things using wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objectives of this assignment are to:\n",
    "* Write a program to detect **partial syntactic** structures\n",
    "* Extract **named entities** and link them to real things using **Wikipedia**\n",
    "* Understand the principles of **supervised machine learning** techniques applied to language processing\n",
    "* Use a popular machine learning toolkit: **scikit-learn**\n",
    "* Write a short report of 2 to 3 pages on the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a training and a test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As annotated data and annotation scheme, you will use the data available from [CoNLL 2000](https://www.clips.uantwerpen.be/conll2000/chunking/).\n",
    "* Download both the training and test sets and decompress them.\n",
    "* Local copies are also available here: [train.txt](https://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/train.txt) and [test.txt](https://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/test.txt)\n",
    "* Read the description of the CoNLL 2000 task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the corpus <a name='t1'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to adjust the paths to load the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_file = '../../corpus/conll2000/train.txt'\n",
    "train_file = 'conll2000/train.txt'\n",
    "#test_file = '../../corpus/conll2000/test.txt'\n",
    "test_file = 'conll2000/test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the functions below to load the datasets. They store the corpus in a **list of sentences**. Each sentence is a list of rows, where each row is a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(file):\n",
    "    \"\"\"\n",
    "    Creates a list of sentences from the corpus\n",
    "    Each sentence is a string\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    f = open(file).read().strip()\n",
    "    sentences = f.split('\\n\\n')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_rows(sentences, column_names):\n",
    "    \"\"\"\n",
    "    Creates a list of sentence where each sentence is a list of lines\n",
    "    Each line is a dictionary of columns\n",
    "    :param sentences:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        rows = sentence.split('\\n')\n",
    "        sentence = [dict(zip(column_names, row.split())) for row in rows]\n",
    "        new_sentences.append(sentence)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The train and test data consist of **three columns** separated by spaces. Each word has been put on a separate line and there is an empty line after each sentence. The first column contains the **current word**, the second its **part-of-speech** (pos) tag as derived by the Brill tagger and the third its **chunk** tag as derived from the WSJ corpus. \n",
    "\n",
    "The **chunk tags** contain the name of the chunk type, for example I-NP for noun phrase words and I-VP for verb phrase words. Most chunk types have **two types of chunk** tags, B-CHUNK for the first word of the chunk and I-CHUNK for each other word in the chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CoNLL 2000 files have three columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['form', 'pos', 'chunk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the corpus, **the training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       "  {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of sentences\n",
    "train_sentences = read_sentences(train_file)\n",
    "\n",
    "# create a list of sentences\n",
    "# each sentence is a list of lines\n",
    "# each line is a dictionary of columns\n",
    "train_corpus = split_rows(train_sentences, column_names)\n",
    "\n",
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline chunker <a name='t2'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most **statistical algorithms** for language processing start with a so-called baseline. The **baseline performance** corresponds to the application of a minimal technique that is used to assess the difficulty of a task and for comparison with further programs.\n",
    "\n",
    "You will implement the **baseline** proposed by the organizers of the\n",
    "        <a href=\"https://www.clips.uantwerpen.be/conll2000/chunking/\">CoNLL 2000 shared task</a>, Sect. <i>Results</i>.\n",
    "1. Read it;\n",
    "2. In the report you will tell what do you think of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to count the **parts of speech**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos(corpus):\n",
    "    \"\"\"\n",
    "    Computes the part-of-speech distribution\n",
    "    in a CoNLL 2000 file\n",
    "    :param corpus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pos_cnt = {}\n",
    "    for sentence in corpus:\n",
    "        for row in sentence:\n",
    "            if row['pos'] in pos_cnt:\n",
    "                pos_cnt[row['pos']] += 1\n",
    "            else:\n",
    "                pos_cnt[row['pos']] = 1\n",
    "    return pos_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first collect all the **parts of speech** (pos) and we count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN': 30147,\n",
       " 'IN': 22764,\n",
       " 'DT': 18335,\n",
       " 'VBZ': 4648,\n",
       " 'RB': 6607,\n",
       " 'VBN': 4763,\n",
       " 'TO': 5081,\n",
       " 'VB': 6017,\n",
       " 'JJ': 13085,\n",
       " 'NNS': 13619,\n",
       " 'NNP': 19884,\n",
       " ',': 10770,\n",
       " 'CC': 5372,\n",
       " 'POS': 1769,\n",
       " '.': 8827,\n",
       " 'VBP': 2868,\n",
       " 'VBG': 3272,\n",
       " 'PRP$': 1881,\n",
       " 'CD': 8315,\n",
       " '``': 1531,\n",
       " \"''\": 1493,\n",
       " 'VBD': 6745,\n",
       " 'EX': 206,\n",
       " 'MD': 2167,\n",
       " '#': 36,\n",
       " '(': 274,\n",
       " '$': 1750,\n",
       " ')': 281,\n",
       " 'NNPS': 420,\n",
       " 'PRP': 3820,\n",
       " 'JJS': 374,\n",
       " 'WP': 529,\n",
       " 'RBR': 321,\n",
       " 'JJR': 853,\n",
       " 'WDT': 955,\n",
       " 'WRB': 478,\n",
       " 'RBS': 191,\n",
       " 'PDT': 55,\n",
       " 'RP': 83,\n",
       " ':': 1047,\n",
       " 'FW': 38,\n",
       " 'WP$': 35,\n",
       " 'SYM': 6,\n",
       " 'UH': 15}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_cnt = count_pos(train_corpus)\n",
    "pos_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You will compute the chunk distribution for each **part of speech**. You will use the **training file** to derive the distribution and you will store the results in a **dictionary**. Below, you have an excerpt of the expected results:\n",
    "```\n",
    "{'JJR':\n",
    "{'I-ADVP': 17, 'I-ADJP': 45, 'I-NP': 204, 'B-ADVP': 63,\n",
    "'B-PP': 2, 'B-ADJP': 111, 'B-NP': 382, 'B-VP': 2,\n",
    "'I-VP': 11, 'O': 16},\n",
    "'CC':\n",
    "{'B-ADVP': 3, 'O': 3676, 'I-VP': 104, 'B-CONJP': 6,\n",
    "'I-ADVP': 30, 'I-UCP': 2, 'I-PP': 24, 'I-ADJP': 26,\n",
    "'I-NP': 1409, 'B-ADJP': 2, 'B-NP': 18, 'B-PP': 70,\n",
    "'I-PRT': 1, 'B-VP': 1},\n",
    "'NN':\n",
    "{'B-LST': 2, 'I-INTJ': 2, 'B-ADVP': 38, 'O': 37,\n",
    "'I-ADVP': 11, 'B-INTJ': 1, 'I-UCP': 2, 'B-UCP': 2,\n",
    "'I-VP': 77, 'B-PRT': 2, 'I-ADJP': 41, 'I-NP': 24456,\n",
    "'B-ADJP': 44, 'B-NP': 5160, 'B-PP': 15, 'B-VP': 257},\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# a dictionary to store the results in\n",
    "chunk_dist = {k: {} for k in pos_cnt.keys( )}\n",
    "\n",
    "# compute the chunk distribution for each part of speech\n",
    "for sentence in train_corpus:\n",
    "    for row in sentence:\n",
    "        pos = row['pos']\n",
    "        chunk = row['chunk']\n",
    "        \n",
    "        if chunk in chunk_dist[pos]:\n",
    "            chunk_dist[pos][chunk] += 1\n",
    "        else:\n",
    "            chunk_dist[pos][chunk] = 1\n",
    "        \n",
    "            \n",
    "#print(chunk_dist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-NP': 5160,\n",
       " 'I-NP': 24456,\n",
       " 'B-VP': 257,\n",
       " 'B-ADJP': 44,\n",
       " 'B-ADVP': 38,\n",
       " 'O': 37,\n",
       " 'B-PP': 15,\n",
       " 'I-ADVP': 11,\n",
       " 'I-ADJP': 41,\n",
       " 'I-VP': 77,\n",
       " 'B-INTJ': 1,\n",
       " 'B-LST': 2,\n",
       " 'B-UCP': 2,\n",
       " 'I-UCP': 2,\n",
       " 'B-PRT': 2,\n",
       " 'I-INTJ': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_dist['NN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the POS-chunk associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each part of speech, select the **best association**. In the example above, you will have (NN, I-NP) as it is the most frequent. You will store the results in a **dictionary** that you will call `pos_chunk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# a dictionary to store the best association in\n",
    "pos_chunk = {} \n",
    "\n",
    "for pos in chunk_dist:\n",
    "    most_freq = 0\n",
    "    most_freq_chunk = ''\n",
    "    for chunk in chunk_dist[pos]:\n",
    "        if chunk_dist[pos][chunk] >= most_freq:\n",
    "            most_freq = chunk_dist[pos][chunk]\n",
    "            most_freq_chunk = chunk\n",
    "        \n",
    "    # select the best association for each pos    \n",
    "    pos_chunk[pos] = most_freq_chunk\n",
    "        \n",
    "            \n",
    "#print(pos_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I-NP'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_chunk['NN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the resulting **associations**, apply your chunker to the **test file**. You will write a `predict(model, corpus)` function, where `model` will be your associations and `corpus`, the test corpus. You will format the test corpus as a **dictionary**, where you will add a `pchunk` key for each row with a value that will correspond to the **predicted chunk**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def predict(model, corpus):\n",
    "    for sentence in corpus:\n",
    "        for row in sentence:\n",
    "            # add a pchunk key for each row\n",
    "            # with a value that will correspond to the predicted chunk\n",
    "            row['pchunk'] = model[row['pos']]\n",
    "            \n",
    "    return corpus\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the **test corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = read_sentences(test_file)\n",
    "test_corpus = split_rows(test_sentences, column_names)\n",
    "test_corpus[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict the groups. You should have added a `pchunk` key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP', 'pchunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply def predict(model, corpus) where model is the best association\n",
    "predicted_test_corpus = predict(pos_chunk, test_corpus)\n",
    "predicted_test_corpus[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the **performance of the baseline** with the tag accuracy: the percentage of words that receive the correct tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(predicted):\n",
    "    \"\"\"\n",
    "    Evaluates the predicted chunk accuracy\n",
    "    :param predicted:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    word_cnt = 0\n",
    "    correct = 0\n",
    "    for sentence in predicted:\n",
    "        for row in sentence:\n",
    "            word_cnt += 1\n",
    "            if row['chunk'] == row['pchunk']:\n",
    "                correct += 1\n",
    "    return correct / word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7729066846782194"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = eval(predicted_test_corpus)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CoNLL evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is very misleading as it is **biased** by the most frequent tags. It is not a good way to evaluate chunking. Instead, CoNLL computes the **F1 score** of all the chunks with a specific **evaluation script**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the **CoNLL evaluation script**, you will store your results in an **output file** that has four columns. The three first columns will be the input columns from the test file: \n",
    "* word, \n",
    "* part of speech, and \n",
    "* gold-standard chunk. \n",
    "\n",
    "You will append the **predicted chunk** as the 4th column. Your output file should look like the excerpt below:\n",
    "```\n",
    "Rockwell NNP B-NP I-NP\n",
    "International NNP I-NP I-NP\n",
    "Corp. NNP I-NP I-NP\n",
    "'s POS B-NP B-NP\n",
    "Tulsa NNP I-NP I-NP\n",
    "unit NN I-NP I-NP\n",
    "said VBD B-VP B-VP\n",
    "it PRP B-NP B-NP\n",
    "```\n",
    "The separator is the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use a `save_results(output_dict, keys, output_file)` function, where the keys will be `['form', 'pos', 'chunk', 'pchunk']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['form', 'pos', 'chunk', 'pchunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(output_dict, keys, output_file):\n",
    "    f_out = open(output_file, 'w')\n",
    "    # We write the word (form), part of speech (pos),\n",
    "    # gold-standard chunk (chunk), and predicted chunk (pchunk)\n",
    "    for sentence in output_dict:\n",
    "        for row in sentence:\n",
    "            for key in keys:\n",
    "                f_out.write(row[key] + ' ')\n",
    "            f_out.write('\\n')\n",
    "        f_out.write('\\n')\n",
    "    f_out.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(predicted_test_corpus, keys, 'out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **CoNLL 2000 evaluation script** will use these two last columns, **chunk** and **predicted chunk**, to compute the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate your results, you have two options:\n",
    "1. Use the original conlleval script here:  <a href=\"https://www.clips.uantwerpen.be/conll2000/chunking/\"><tt>conlleval.txt</tt></a>.\n",
    "2. Use a Python translation of it. \n",
    "\n",
    "You will use the second option and you will **describe the results you obtained in your report**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Python translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the script with:\n",
    "```\n",
    "pip install conlleval\n",
    "```\n",
    "from https://github.com/kaniblu/conlleval\n",
    "and run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conlleval\n",
    "lines = open('out').read().splitlines()\n",
    "\n",
    "# use CoNLL 2000 evaluate\n",
    "res = conlleval.evaluate(lines)\n",
    "baseline_score = res['overall']['chunks']['evals']['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.770671072299583"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The official script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to double-check your results with the **original CoNLL script**. It is more complex to use however:\n",
    "* <tt>conlleval.txt</tt> is the official CoNLL Perl script. It expects the two last columns of the test set to be the manually assigned chunk (gold standard) and the predicted chunk.\n",
    "* <tt>conlleval.txt</tt> was written for Unix and if you run Windows, you will have to use a terminal command. In the File menu of the notebook, select New and then Terminal.\n",
    "* Start it like this: ` $ conlleval.txt <out` where the `out` file contains both the gold and predicted chunk tags. `conlleval.txt` is a Perl script.\n",
    "* Perl is installed on most Unix distributions. If it is not installed on your machine, you need to install it. Make also sure that you have the execution rights. Otherwise change them with: `$ chmod +x conlleval.txt`\n",
    "* The `conlleval.txt` script expects the new lines to be `\\n` as in Unix. If you run your Python program on Windows, your new lines will be `\\r\\n`. To have the correct new lines, add this parameter to `open()`: `newline='\\n’` like this: `f_out = open('out', ‘w’, newline='\\n’)`\n",
    "* The complete description of the CoNLL 2000 evaluation script is available here: [https://www.clips.uantwerpen.be/conll2000/chunking/output.html](https://www.clips.uantwerpen.be/conll2000/chunking/output.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Machine Learning: A first ML program <a name='t3'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will apply and explore a machine-learning program.\n",
    "\n",
    "The program that won the CoNLL 2000 shared task (Kudoh and Matsumoto, 2000) used a **window of five words** around the chunk tag to identify, $c_i$. They built a **feature vector** consisting of:\n",
    "1. The values of the **five words** in this window: $w_{i-2}, w_{i-1}, w_{i}, w_{i+1}, w_{i+2}$\n",
    "2. The values of the **five pos** parts of speech in this window: $t_{i-2}, t_{i-1}, t_{i}, t_{i+1}, t_{i+2}$\n",
    "3. The values of the **two previous chunk** tags in the first part of the window: $c_{i-2}, c_{i-1}$\n",
    "\n",
    "The two last parameters (3.) are said to be **dynamic** because the program computes them at run-time. Read [Kudoh and Matsumoto's paper](http://www.clips.uantwerpen.be/conll2000/pdf/14244kud.pdf) and the [Yamcha](http://www.chasen.org/~taku/software/yamcha/) software site.\n",
    "\n",
    "You will start with a given code that uses the two first sets of features (1. and 2.) and add yourself the last one (3.) to improve the performance of your chunker. Kudoh and Matsumoto trained a classifier based on **support vector machines**. You will use **logistic regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import os\n",
    "import requests\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first function to extract features **from one sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_sent_static(sentence, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Extract the features from one sentence\n",
    "    returns X and y, where X is a list of dictionaries and\n",
    "    y is a list of symbols\n",
    "    :param sentence: string containing the CoNLL structure of a sentence\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # We pad the sentence to extract the context window more easily\n",
    "    start = [{'form': 'BOS', 'pos': 'BOS', 'chunk': 'BOS'}]\n",
    "    end = [{'form': 'EOS', 'pos': 'EOS', 'chunk': 'EOS'}]\n",
    "    start *= w_size\n",
    "    end *= w_size\n",
    "    padded_sentence = start + sentence\n",
    "    padded_sentence += end\n",
    "\n",
    "    # We extract the features and the classes\n",
    "    # X contains is a list of features, where each feature vector is a dictionary\n",
    "    # y is the list of classes\n",
    "    X = list()\n",
    "    y = list()\n",
    "    for i in range(len(padded_sentence) - 2 * w_size):\n",
    "        # x is a row of X\n",
    "        x = list()\n",
    "        \n",
    "        # The words in lower case\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j]['form'].lower())\n",
    "            \n",
    "        # The POS\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j]['pos'])\n",
    "            \n",
    "        # The chunks (Up to the word)\n",
    "        \"\"\"\n",
    "        for j in range(w_size):\n",
    "            feature_line.append(padded_sentence[i + j]['chunk'])\n",
    "        \"\"\"\n",
    "        # We represent the feature vector as a dictionary\n",
    "        X.append(dict(zip(feature_names, x)))\n",
    "        \n",
    "        # The classes are stored in a list\n",
    "        y.append(padded_sentence[i + w_size]['chunk'])\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And **from all the sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_static(sentences, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Builds X matrix and y vector\n",
    "    X is a list of dictionaries and y is a list\n",
    "    :param sentences:\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    for sentence in sentences:\n",
    "        X, y = extract_features_sent_static(sentence, w_size, feature_names)\n",
    "        X_l.extend(X)\n",
    "        y_l.extend(y)\n",
    "    return X_l, y_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the window and the names of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_size = 2  # The size of the context window to the left and right of the word\n",
    "feature_names = ['word_n2', 'word_n1', 'word', 'word_p1', 'word_p2',\n",
    "                 'pos_n2', 'pos_n1', 'pos', 'pos_p1', 'pos_p2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the **training** corpus and format it as a **dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = read_sentences(train_file)\n",
    "train_corpus = split_rows(train_sentences, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       "  {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word_n2': 'bos',\n",
       "  'word_n1': 'bos',\n",
       "  'word': 'confidence',\n",
       "  'word_p1': 'in',\n",
       "  'word_p2': 'the',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'BOS',\n",
       "  'pos': 'NN',\n",
       "  'pos_p1': 'IN',\n",
       "  'pos_p2': 'DT'},\n",
       " {'word_n2': 'bos',\n",
       "  'word_n1': 'confidence',\n",
       "  'word': 'in',\n",
       "  'word_p1': 'the',\n",
       "  'word_p2': 'pound',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'NN',\n",
       "  'pos': 'IN',\n",
       "  'pos_p1': 'DT',\n",
       "  'pos_p2': 'NN'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict, y = extract_features_static(train_corpus, w_size, feature_names)\n",
    "X_dict[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-NP', 'B-PP']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the feature matrix and carry out a one-hot encoding\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = linear_model.LogisticRegression()\n",
    "model = classifier.fit(X, y)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the sentences and create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': '200', 'pos': 'CD', 'chunk': 'B-NP'},\n",
       "  {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = read_sentences(test_file)\n",
    "test_corpus = split_rows(test_sentences, column_names)\n",
    "test_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the **features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word_n2': 'bos',\n",
       "  'word_n1': 'bos',\n",
       "  'word': 'rockwell',\n",
       "  'word_p1': 'international',\n",
       "  'word_p2': 'corp.',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'BOS',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'NNP'},\n",
       " {'word_n2': 'bos',\n",
       "  'word_n1': 'rockwell',\n",
       "  'word': 'international',\n",
       "  'word_p1': 'corp.',\n",
       "  'word_p2': \"'s\",\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'NNP',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'POS'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dict, y_test = extract_features_static(test_corpus, w_size, feature_names)\n",
    "X_test_dict[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-NP', 'I-NP']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **vectorize** the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vec.transform(X_test_dict)  # Possible to add: .toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we **predict** the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B-NP', 'I-NP'], dtype='<U7')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predicted = classifier.predict(X_test)\n",
    "y_test_predicted[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add the **predicted chunks** to the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inx = 0\n",
    "for sentence in test_corpus:\n",
    "    for word in sentence:\n",
    "        word['pchunk'] = y_test_predicted[inx]\n",
    "        inx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **index** sould be equal to the length of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47377"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inx)\n",
    "len(y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP', 'pchunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}],\n",
       " [{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR', 'pchunk': 'B-SBAR'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP', 'pchunk': 'I-VP'},\n",
       "  {'form': '200', 'pos': 'CD', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(test_corpus, keys, 'out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('out').read().splitlines()\n",
    "res = conlleval.evaluate(lines)\n",
    "simple_ml_score = res['overall']['chunks']['evals']['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.915119639107748"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_ml_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question on the ML program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the **feature vector** that corresponds to the <tt>ml_chunker.py</tt> program? Is it the same Kudoh\n",
    "    and Matsumoto used in their experiment?\n",
    "2. What is the **performance** of the chunker?\n",
    "3. Remove the lexical features (the words) from the feature vector and measure the performance. You should\n",
    "    observe a decrease.\n",
    "4. What is the **classifier** used in the program? \n",
    "5. As an optional task, you may try **two other classifiers from sklearn** and measure their performance: decision trees, perceptron, support vector machines, etc. Be aware that support vector machines take a long time to train: up to one hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Machine Learning: Adding all the features from Kudoh and Matsumoto <a name='t4'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complement the feature vector used in the previous section with the **two dynamic features**, $c_{i-2}, c_{i-1}$, and train a new model. You will need to write a new `extract_features_sent_dyn` and `predict` functions. \n",
    "In his experiments, your teacher obtained a F1 score of 92.65 with **logistic regression** and the default parameters from sklearn, i.e. `linear_model.LogisticRegression()`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A frequent mistake in the labs** is to use the gold-standard chunks from the test set. Be aware that  when you predict the test set, you do not know the dynamic features in advance and you must  not use the ones from the test file. You will **use the two previous chunk tags that you have predicted**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to reach a **global F1 score** of 92 to pass this laboratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### write your code here\n",
    "#  function to extract dynamic features from one sentence\n",
    "def extract_features_sent_dyn(sentence, w_size, feature_names, prediction=False):\n",
    "    \"\"\"\n",
    "    Extract the dynamic features from one sentence\n",
    "    returns X and y, where X is a list of dictionaries and\n",
    "    y is a list of symbols\n",
    "    :param sentence: string containing the CoNLL structure of a sentence\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # We pad the sentence to extract the context window more easily\n",
    "    start = [{'form': 'BOS', 'pos': 'BOS', 'chunk': 'BOS'}]\n",
    "    end = [{'form': 'EOS', 'pos': 'EOS', 'chunk': 'EOS'}]\n",
    "    start *= w_size\n",
    "    end *= w_size\n",
    "    padded_sentence = start + sentence\n",
    "    padded_sentence += end\n",
    "\n",
    "    # We extract the dynamic features and the classes\n",
    "    # X contains is a list of features, where each feature vector is a dictionary\n",
    "    # y is the list of classes\n",
    "    X = list()\n",
    "    y = list()\n",
    "    for i in range(len(padded_sentence) - 2 * w_size):\n",
    "        # x is a row of X\n",
    "        x = list()\n",
    "        \n",
    "        # The words in lower case\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j]['form'].lower())\n",
    "            \n",
    "        # The POS\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j]['pos'])\n",
    "            \n",
    "        # The chunks (Up to the word)\n",
    "        \"\"\"\n",
    "        for j in range(w_size):\n",
    "            feature_line.append(padded_sentence[i + j]['chunk'])\n",
    "        \"\"\"\n",
    "        if not prediction:\n",
    "            for j in range(w_size):\n",
    "                x.append(padded_sentence[i+j]['chunk'])\n",
    "                \n",
    "        # We represent the feature vector as a dictionary\n",
    "        X.append(dict(zip(feature_names, x)))\n",
    "        \n",
    "        # The classes are stored in a list\n",
    "        y.append(padded_sentence[i + w_size]['chunk'])\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_dyn(sentences, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Builds X matrix and y vector\n",
    "    X is a list of dictionaries and y is a list\n",
    "    :param sentences:\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    for sentence in sentences:\n",
    "        X, y = extract_features_sent_dyn(sentence, w_size, feature_names)\n",
    "        X_l.extend(X)\n",
    "        y_l.extend(y)\n",
    "    return X_l, y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_dyn = ['word_n2', 'word_n1', 'word', 'word_p1', 'word_p2',\n",
    "                     'pos_n2', 'pos_n1', 'pos', 'pos_p1', 'pos_p2', 'chunk_n2',\n",
    "                     'chunk_n1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The goal of this task is to come forward with machine learning methods which \n",
    "after a training phase can recognize the **chunk segmentation** of the test data as well as \n",
    "possible. The training data can be used for training the text chunker. \n",
    "\n",
    "The chunkers will be evaluated with the **F rate**, which is a combination of the **precision** \n",
    "and **recall** rates: $F = 2*precision*recall / (recall + precision)$ [Rij79]. The precision and recall numbers will be computed over all types of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       "  {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = read_sentences(train_file)\n",
    "train_corpus = split_rows(train_sentences, column_names)\n",
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dict, y = extract_features_dyn(train_corpus, w_size, feature_names_dyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word_n2': 'bos',\n",
       "  'word_n1': 'bos',\n",
       "  'word': 'confidence',\n",
       "  'word_p1': 'in',\n",
       "  'word_p2': 'the',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'BOS',\n",
       "  'pos': 'NN',\n",
       "  'pos_p1': 'IN',\n",
       "  'pos_p2': 'DT',\n",
       "  'chunk_n2': 'BOS',\n",
       "  'chunk_n1': 'BOS'},\n",
       " {'word_n2': 'bos',\n",
       "  'word_n1': 'confidence',\n",
       "  'word': 'in',\n",
       "  'word_p1': 'the',\n",
       "  'word_p2': 'pound',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'NN',\n",
       "  'pos': 'IN',\n",
       "  'pos_p1': 'DT',\n",
       "  'pos_p2': 'NN',\n",
       "  'chunk_n2': 'BOS',\n",
       "  'chunk_n1': 'B-NP'},\n",
       " {'word_n2': 'confidence',\n",
       "  'word_n1': 'in',\n",
       "  'word': 'the',\n",
       "  'word_p1': 'pound',\n",
       "  'word_p2': 'is',\n",
       "  'pos_n2': 'NN',\n",
       "  'pos_n1': 'IN',\n",
       "  'pos': 'DT',\n",
       "  'pos_p1': 'NN',\n",
       "  'pos_p2': 'VBZ',\n",
       "  'chunk_n2': 'B-NP',\n",
       "  'chunk_n1': 'B-PP'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now **vectorize the training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "# Vectorize the feature matrix and carry out a one-hot encoding\n",
    "vec_dyn = DictVectorizer(sparse=True)\n",
    "X = vec_dyn.fit_transform(X_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And fit the **dynamic model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "classifier = linear_model.LogisticRegression()\n",
    "model = classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will finally predict the test set. We **load the corpus** again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': '200', 'pos': 'CD', 'chunk': 'B-NP'},\n",
       "  {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We read the sentences and create a dictionary\n",
    "test_sentences = read_sentences(test_file)\n",
    "test_corpus = split_rows(test_sentences, column_names)\n",
    "test_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us extract the **static features** from **one sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word_n2': 'bos',\n",
       "  'word_n1': 'bos',\n",
       "  'word': 'rockwell',\n",
       "  'word_p1': 'international',\n",
       "  'word_p2': 'corp.',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'BOS',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'NNP'},\n",
       " {'word_n2': 'bos',\n",
       "  'word_n1': 'rockwell',\n",
       "  'word': 'international',\n",
       "  'word_p1': 'corp.',\n",
       "  'word_p2': \"'s\",\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'NNP',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'POS'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dict, y_test = extract_features_static([test_corpus[0]], w_size, feature_names)\n",
    "X_test_dict[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This $\\mathbf{X}\\_{\\textrm{dict}}$ is incomplete. For the prediction, we need to **reinject dynamically the two previously predicted tags** to have the **full feature vector**. Write this code here. \n",
    "\n",
    "This part is probably the most difficult of the lab. You may want to write it first for **one sentence**, and then for the test corpus. The prediction will take a longer time and you may want to include a **progress bar** with this snippet: \n",
    "```\n",
    "from tqdm import tqdm\n",
    "for test_sentence in tqdm(test_corpus):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Since the **chunk labels** are not given in the test data, they are decided\n",
    "dynamically during the tagging of chunk labels. This technique can be regarded as a sort of **Dynamic Programming (DP) matching**, in which the best answer is searched by maximizing the\n",
    "total certainty score for the combination of tags. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment1: Prediction for one sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test_predicted_dyn = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#write your code here\n",
    "#from tqdm import tqdm\n",
    "\n",
    "#for test_sentence in tqdm(test_corpus):\n",
    "#for test_sentence in test_corpus:\n",
    "\n",
    "#extract the features dynamically\n",
    "X_test_dict, y_test = extract_features_sent_dyn(test_sentence, w_size, \n",
    "                                                feature_names, True)\n",
    "#X_test_dict, y_test = extract_features_dyn(test_corpus, w_size, feature_names)\n",
    "\n",
    "test_data = X_test_dict.copy()\n",
    "\n",
    "test_data[0]['chunk_n2'] = 'BOS'\n",
    "test_data[0]['chunk_n1'] = 'BOS'\n",
    "test_data[1]['chunk_n2'] = 'BOS'\n",
    "\n",
    "data_length = len(test_data)\n",
    "\n",
    "for i in range(data_length-1):\n",
    "\n",
    "    vector = vec_dyn.transform(test_data[i])\n",
    "    #print('vector X', vector.shape)\n",
    "    chunk_prediction = model.predict(vector)[0]  # XXX TOP SECRET !\n",
    "    #chunk_prediction = model.predict(vector)\n",
    "    #print(chunk_prediction)\n",
    "    #print(type(chunk_prediction))\n",
    "    \n",
    "    test_data[i+1]['chunk_n1'] = chunk_prediction\n",
    "    \n",
    "    if(i<data_length-2):\n",
    "        test_data[i+2]['chunk_n2'] = chunk_prediction\n",
    "        \n",
    "y_test_predicted_dyn = model.predict(vec_dyn.transform(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(y_test_predicted_dyn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_dict[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_predicted_dyn[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment2: Prediction for one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_dyn = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization of the two previously predicted chunks\n",
    "y_test_predicted_labels = ['BOS', 'BOS']\n",
    "#y_test_predicted_labels = []\n",
    "\n",
    "X_test_dict, y_test = extract_features_sent_dyn(test_corpus[0], w_size, feature_names, True)\n",
    "X_test_dict[:2]\n",
    "\n",
    "    \n",
    "#inject the two previously predicted tags\n",
    "for word in X_test_dict:\n",
    "    word['chunk_n2'] = y_test_predicted_labels[-2]\n",
    "    word['chunk_n1'] = y_test_predicted_labels[-1]\n",
    "    #print('chunks ', x)\n",
    "\n",
    "        \n",
    "    # vectorize the test sentence/features\n",
    "    #vec_dyn = DictVectorizer(sparse=True)\n",
    "    X_test = vec_dyn.transform(word)\n",
    "        \n",
    "    # predict the chunks in the test set\n",
    "    predicted = model.predict(X_test)\n",
    "    #predicted = model.predict(X_test)[0]\n",
    "    #print(y_test_predicted_dyn[0])\n",
    "    #print('predicted', predicted)\n",
    "        \n",
    "    \n",
    "    #print('labels 1 and 2 before ', y_test_predicted_labels)\n",
    "    y_test_predicted_dyn.append(predicted[0])\n",
    "    \n",
    "    # update the chunk labels c(i-2) and c(i-1)\n",
    "    y_test_predicted_labels[0] = y_test_predicted_labels[1]\n",
    "    y_test_predicted_labels[1] = predicted[0]\n",
    "    #print('labels 1-2 after', y_test_predicted_labels)\n",
    "        \n",
    " #append the predicted chunks as a last column\n",
    "#y_test_predicted_labels = y_test_predicted_dyn[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_dict[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(y_test_predicted_dyn)) # 28\n",
    "#print(y_test_predicted_dyn[:3])\n",
    "# ['B-NP' 'I-NP' 'I-NP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_dyn = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2012/2012 [00:22<00:00, 88.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "from tqdm import tqdm\n",
    "\n",
    "for test_sentence in tqdm(test_corpus):\n",
    "#for test_sentence in test_corpus:\n",
    "\n",
    "    # initialize the two previously predicted chunks\n",
    "    c = ['BOS', 'BOS']\n",
    "    \n",
    "    # extract the features dynamically\n",
    "    X_test_dict, y_test = extract_features_sent_dyn(test_sentence, w_size, \n",
    "                                                    feature_names, True)\n",
    "   # X_test_dict, y_test = extract_features_dyn(test_sentence, w_size, feature_names)\n",
    "    \n",
    "    # Iterate over the words in each sentence\n",
    "    # inject the two previously predicted tags\n",
    "    for word in X_test_dict:\n",
    "        # overwrite the golden chunks from previous tag\n",
    "        word['chunk_n2'] = c[-2]\n",
    "        word['chunk_n1'] = c[-1]\n",
    "        #print('chunks ', )word\n",
    "        \n",
    "        # vectorize the test sentence/features\n",
    "        X_test = vec_dyn.transform(word)\n",
    "        \n",
    "        # predict the chunk in this iteration\n",
    "        #y_test_predicted_dyn = classifier.predict(X_test)\n",
    "        predicted_chunk = model.predict(X_test)\n",
    "        \n",
    "        # append the result\n",
    "        y_test_predicted_dyn.append(predicted_chunk[0])\n",
    "        \n",
    "        # update the chunk labels for future use\n",
    "        #y_test_predicted_labels.append(y_test_predicted_dyn[0])\n",
    "        #c = predicted_chunk[0:2]\n",
    "        c[-2] = c[-1]\n",
    "        c[-1] = predicted_chunk[0]\n",
    "        #print('The two previously predicted tags ', c)\n",
    "        \n",
    "    # append the predicted chunks as a last column\n",
    "    #y_test_predicted_labels = y_test_predicted_dyn[2:]\n",
    "    #print(y_test_predicted_labels)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_dict[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['B-NP', 'I-NP', 'I-NP']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(y_test_predicted_dyn))\n",
    "y_test_predicted_dyn[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "inx = 0\n",
    "for sentence in test_corpus:\n",
    "    for word in sentence:\n",
    "        word['pchunk'] = y_test_predicted_dyn[inx]\n",
    "        inx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(test_corpus, keys, 'out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9231961786642086"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = open('out').read().splitlines()\n",
    "res = conlleval.evaluate(lines)\n",
    "improved_ml_score = res['overall']['chunks']['evals']['f1']\n",
    "improved_ml_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an optional task, you can try to improve the score with **beam search**. If you know this technique, apply it using the probability output of **logistic regression**.\n",
    "\n",
    "With the same classifier and a beam diameter of 5, your teacher obtained 92.87."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the entities <a name='t5'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now collect all the **named entities** from the training set, defined as **NP chunks** and starting with a `NNP` (proper noun) or a `NNPS` (proper noun, plural) tag. As an example, in the first sentence of `train_corpus`, you will extract `('September', )` and `('July', 'and', 'August')`. You will set all the **tuples** in a **set** that you will call `ne_set`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Parts of speech are useful features for labeling **named entities**\n",
    "like people or organizations in **information extraction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       "  {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'But', 'pos': 'CC', 'chunk': 'O'},\n",
       "  {'form': 'analysts', 'pos': 'NNS', 'chunk': 'B-NP'},\n",
       "  {'form': 'reckon', 'pos': 'VBP', 'chunk': 'B-VP'},\n",
       "  {'form': 'underlying', 'pos': 'VBG', 'chunk': 'B-NP'},\n",
       "  {'form': 'support', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'been', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'eroded', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'by', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'chancellor', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'failure', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'announce', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'any', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'new', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'measures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'his', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'Mansion', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'House', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'speech', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'last', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'Thursday', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'This', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'increased', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'risk', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'government', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'being', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       "  {'form': 'forced', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'increase', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'base', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'rates', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': '16', 'pos': 'CD', 'chunk': 'B-NP'},\n",
       "  {'form': '%', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'their', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'current', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': '15', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': '%', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'level', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'defend', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'economists', 'pos': 'NNS', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'O'},\n",
       "  {'form': 'foreign', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'exchange', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'market', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'analysts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'say', 'pos': 'VBP', 'chunk': 'B-VP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': '``', 'pos': '``', 'chunk': 'O'},\n",
       "  {'form': 'The', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'risks', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'bad', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'figure', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'are', 'pos': 'VBP', 'chunk': 'B-VP'},\n",
       "  {'form': 'very', 'pos': 'RB', 'chunk': 'B-ADVP'},\n",
       "  {'form': 'heavily', 'pos': 'RB', 'chunk': 'I-ADVP'},\n",
       "  {'form': 'on', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'down', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'side', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': \"''\", 'pos': \"''\", 'chunk': 'O'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'Chris', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Dillow', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'senior', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'U.K.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'economist', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'at', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Nomura', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Research', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Institute', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': '``', 'pos': '``', 'chunk': 'O'},\n",
       "  {'form': 'If', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'there', 'pos': 'EX', 'chunk': 'B-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'bad', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'number', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'there', 'pos': 'EX', 'chunk': 'B-NP'},\n",
       "  {'form': 'could', 'pos': 'MD', 'chunk': 'B-VP'},\n",
       "  {'form': 'be', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'an', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'awful', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'lot', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'pressure', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': \"''\", 'pos': \"''\", 'chunk': 'O'},\n",
       "  {'form': 'noted', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'Simon', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Briscoe', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'U.K.', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'economist', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Midland', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Montagu', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Midland', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Bank', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'PLC', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Forecasts', 'pos': 'NNS', 'chunk': 'B-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'range', 'pos': 'VBP', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'B-ADVP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'but', 'pos': 'CC', 'chunk': 'O'},\n",
       "  {'form': 'few', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'economists', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'expect', 'pos': 'VBP', 'chunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'data', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'very', 'pos': 'RB', 'chunk': 'I-NP'},\n",
       "  {'form': 'marked', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'O'},\n",
       "  {'form': '#', 'pos': '#', 'chunk': 'O'},\n",
       "  {'form': '2', 'pos': 'CD', 'chunk': 'O'},\n",
       "  {'form': 'billion', 'pos': 'CD', 'chunk': 'O'},\n",
       "  {'form': '-LRB-', 'pos': '(', 'chunk': 'O'},\n",
       "  {'form': '$', 'pos': '$', 'chunk': 'B-ADJP'},\n",
       "  {'form': '3.2', 'pos': 'CD', 'chunk': 'O'},\n",
       "  {'form': 'billion', 'pos': 'CD', 'chunk': 'O'},\n",
       "  {'form': '-RRB-', 'pos': ')', 'chunk': 'O'},\n",
       "  {'form': 'deficit', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'current', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'account', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'reported', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'The', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'O'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': '#', 'pos': '#', 'chunk': 'I-NP'},\n",
       "  {'form': '2.2', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'billion', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'gap', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'registered', 'pos': 'VBN', 'chunk': 'B-VP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'are', 'pos': 'VBP', 'chunk': 'B-VP'},\n",
       "  {'form': 'topped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'only', 'pos': 'RB', 'chunk': 'B-ADVP'},\n",
       "  {'form': 'by', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': '#', 'pos': '#', 'chunk': 'I-NP'},\n",
       "  {'form': '2.3', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'billion', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'October', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': '1988', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Sanjay', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Joshi', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'European', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'economist', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'at', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Baring', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Brothers', 'pos': 'NNPS', 'chunk': 'I-NP'},\n",
       "  {'form': '&', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'there', 'pos': 'EX', 'chunk': 'B-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'no', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sign', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'that', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'Britain', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'manufacturing', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'industry', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'transforming', 'pos': 'VBG', 'chunk': 'I-VP'},\n",
       "  {'form': 'itself', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'boost', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'exports', 'pos': 'NNS', 'chunk': 'B-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'At', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'same', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'time', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'he', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'remains', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'fairly', 'pos': 'RB', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'pessimistic', 'pos': 'JJ', 'chunk': 'I-ADJP'},\n",
       "  {'form': 'about', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'outlook', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'imports', 'pos': 'NNS', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'given', 'pos': 'VBN', 'chunk': 'B-PP'},\n",
       "  {'form': 'continued', 'pos': 'VBD', 'chunk': 'B-NP'},\n",
       "  {'form': 'high', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'consumer', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'capital', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'goods', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'inflows', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write a **two-pass procedure**. For each sentence of the corpus:\n",
    "1. In the first pass, you will collect the **start indices** of the noun groups which are also proper nouns. For the first sentence, it will result in the list `[16, 30]`;\n",
    "2. In the second pass, you will collect **the segments**, starting at each index. For the first sentence, it will result in the tuples `('September',)`and `('July', 'and', 'August')`\n",
    "\n",
    "Should you have a better solution, please use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect the start indices 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "startidx = []\n",
    "ne_set = set()\n",
    "#ne_set = []\n",
    "#segment = ()\n",
    "#inx = []\n",
    "\n",
    "for sentence in train_corpus:\n",
    "    idx = [i for i in range(len(sentence)) \n",
    "           if (sentence[i]['pos'] in ['NNP', 'NNPS']) \n",
    "           and (sentence[i]['chunk'] =='B-NP')]\n",
    "    #print('idx ', idx)\n",
    "    #if len(idx) != 0:\n",
    "    startidx.append(idx)\n",
    "    \n",
    "    for i in idx:\n",
    "        segment = ()\n",
    "        #print(sentence[i])\n",
    "        segment = (sentence[i]['form'],)\n",
    "        #print('Hi', segment)\n",
    "        #print(sentence[i+1]['chunk'])\n",
    "        \n",
    "        while i<(len(sentence)-1) and sentence[i+1]['chunk'] == 'I-NP':\n",
    "            #if sentence[i+1]['chunk'] == 'I-NP':\n",
    "            segment += (sentence[i+1]['form'],)\n",
    "                #segment = (sentence[i]['form'], sentence[i+1]['form'])\n",
    "                #segment = (sentence[i+1]['form'],)\n",
    "            #else:\n",
    "            #print(sentence[i]['form'])\n",
    "            #segment = (sentence[i]['form'],)\n",
    "            i += 1\n",
    "    \n",
    "        ne_set.add(segment)\n",
    "        #ne_set.append(segment)\n",
    "        #ne_set.update(segment)\n",
    "            #print(inx)\n",
    "            #ne_set.add(word['form'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4348\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "print(len(ne_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Avdel',),\n",
       " ('London', 'brokers', 'UBS', 'Phillips', '&', 'Drew'),\n",
       " ('Reuben', 'Mark'),\n",
       " ('Nynex',),\n",
       " ('Foreign', 'Minister', 'Hans-Dietrich', 'Genscher'),\n",
       " ('Lehman', 'Management', 'Co'),\n",
       " ('Barron',),\n",
       " ('Papua', 'New', 'Guinea'),\n",
       " ('First', 'Gibraltar', 'Bank', 'F.S.B.'),\n",
       " ('Barry', 'R.', 'Ostrager')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ne_set)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if ('Streetspeak',) in list(ne_set):\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting the start indices 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_set2 = set()\n",
    "startIndices = []\n",
    "for sentence in train_corpus:    \n",
    "    inx = []\n",
    "    duplicates = []\n",
    "    #print(len(train_corpus[0]))\n",
    "    #for word in train_corpus[0]:\n",
    "    res = [idx for idx, val in enumerate(sentence) \n",
    "                   if (sentence[idx]['pos'] in ['NNP', 'NNPS']) \n",
    "                   and (sentence[idx]['chunk'] =='B-NP')]\n",
    "    #inx.append(res)\n",
    "    startIndices.append(res)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8936\n",
      "8936\n",
      "[[16, 30], [4], [], [], [20, 27], [20, 23, 26, 32], [37], [11, 22], [0, 6, 17], []]\n",
      "[[16, 30], [4], [], [], [20, 27], [20, 23, 26, 32], [37], [11, 22], [0, 6, 17], []]\n"
     ]
    }
   ],
   "source": [
    "print(len(startIndices))\n",
    "print(len(startidx))\n",
    "print(startIndices[:10])\n",
    "print(startidx[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#sentence = train_corpus[0]\n",
    "for inx in startIndices:\n",
    "#for inx in startidx:\n",
    "    #print(inx)\n",
    "    s = startIndices.index(inx)\n",
    "    #s = startidx.index(inx)\n",
    "    #print(s)\n",
    "    sentence = train_corpus[s]\n",
    "    #segment = ()\n",
    "    for i in inx:\n",
    "        segment = ()\n",
    "        #print(sentence[i])\n",
    "        segment = (sentence[i]['form'],)\n",
    "        #print('Hi', segment)\n",
    "        #print(sentence[i+1]['chunk'])\n",
    "        \n",
    "        while i<(len(sentence)-1) and sentence[i+1]['chunk'] == 'I-NP':\n",
    "            #if sentence[i+1]['chunk'] == 'I-NP':\n",
    "            segment += (sentence[i+1]['form'],)\n",
    "                #segment = (sentence[i]['form'], sentence[i+1]['form'])\n",
    "                #segment = (sentence[i+1]['form'],)\n",
    "            #else:\n",
    "            #print(sentence[i]['form'])\n",
    "            #segment = (sentence[i]['form'],)\n",
    "            i += 1\n",
    "    \n",
    "        ne_set2.add(segment)\n",
    "            #print(inx)\n",
    "            #ne_set.add(word['form'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#print(len(startIndices)) \n",
    "#print(sentence[startIndices[10]])\n",
    "print(len(ne_set2))\n",
    "list(ne_set2)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(ne_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list(ne_set)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a small set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the subsequent experiments faster, you will **limit the dataset** to the entities starting with letter `K`. I chose this letter, because it corresponded to one of the smallest sets. You will call the resulting set: `ne_small_set`. Feel free to use the full set after you have completed this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Avdel',)\n",
      "Hi  A\n",
      "Avdel\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "print(list(ne_set)[0])\n",
    "print('Hi ', str(list(ne_set)[0][0][0]))\n",
    "print(str(list(ne_set)[0][0]))\n",
    "if str(list(ne_set)[0][0][0]) == 'A':\n",
    "    print('yes')\n",
    "else:\n",
    "    print('NO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "#filter_result = list(filter(lambda name: len(name) <= 7, my_names))\n",
    "ne_small_set = list(filter(lambda neK: str(neK[0][0]) == 'K', list(ne_set)))\n",
    "len(ne_small_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Keihin', 'Electric', 'Express', 'Railway', 'Co'),\n",
       " ('Keizaikai', 'Corp.'),\n",
       " ('Ko', 'Shioya'),\n",
       " ('Kenneth', 'H.', 'Olsen'),\n",
       " ('Keizaikai',),\n",
       " ('Kollmorgen',),\n",
       " ('KPMG', 'Peat', 'Marwick'),\n",
       " ('Kenneth', 'Abraham'),\n",
       " ('Kringle', 'fares'),\n",
       " ('Knoxville',),\n",
       " ('Kathie', 'Huff'),\n",
       " ('Kumagai-Gumi',),\n",
       " ('Kawasaki', 'Steel'),\n",
       " ('Krenz',),\n",
       " ('Kevin', 'Logan'),\n",
       " ('Ke', 'Zaishuo'),\n",
       " ('Ky.',),\n",
       " ('Keith', 'Mulrooney'),\n",
       " ('Kansas', 'and', 'Texas'),\n",
       " ('Kean', 'forces'),\n",
       " ('Kobe', 'Steel'),\n",
       " ('Kate', 'Michelman'),\n",
       " ('Kajima',),\n",
       " ('Kremlin', 'wrangling'),\n",
       " ('Kleinwort', 'Benson', 'Government', 'Securities', 'Inc.'),\n",
       " ('Kodak',),\n",
       " ('Kacy', 'McClelland'),\n",
       " ('Kroger', 'Co'),\n",
       " ('Kathy', 'Stanwick'),\n",
       " ('Kleinwort', 'Benson', 'North', 'America'),\n",
       " ('Kurds',),\n",
       " ('K', 'mart', 'Corp.', 'Chairman', 'Joseph', 'E.', 'Antonini'),\n",
       " ('Kansas',),\n",
       " ('Khost',),\n",
       " ('Kurt', 'Hager'),\n",
       " ('Kary', 'Moss'),\n",
       " ('Keefe', ',', 'Bruyette', '&', 'Woods', 'Inc.'),\n",
       " ('Kentucky', 'Fried', 'Chicken', 'stores'),\n",
       " ('Kidder', ',', 'Peabody', '&', 'Co'),\n",
       " ('Kim', 'Schwartz'),\n",
       " ('Ky',),\n",
       " ('Kraft', 'General', 'Foods'),\n",
       " ('Kinney', 'and', 'Foot', 'Locker', 'shoe', 'stores'),\n",
       " ('Kentucky',),\n",
       " ('Kyocera',),\n",
       " ('Kenneth', 'A.', 'Eldred'),\n",
       " ('Kuala', 'Lumpur'),\n",
       " ('Kidder', 'Peabody'),\n",
       " ('Katonah',),\n",
       " ('Karen', 'Olshan'),\n",
       " ('Kathe', 'Dillmann'),\n",
       " ('Kenton',),\n",
       " ('Kirin',),\n",
       " ('Kathie', 'Roberts'),\n",
       " ('KTXL',),\n",
       " ('Kirin', 'Brewery'),\n",
       " ('King', 'Broadcasting', 'Co.'),\n",
       " ('KIM',),\n",
       " ('Kabul',),\n",
       " ('Khmer', 'Rouge', 'camps'),\n",
       " ('Kenneth', 'T.', 'Rosen'),\n",
       " ('Kansas', 'Power'),\n",
       " ('KLM', 'Royal', 'Dutch', 'Airlines'),\n",
       " ('Kursk', 'and', 'Smolensk'),\n",
       " ('Kaitaia',),\n",
       " ('Kidder', ',', 'Peabody', '&', 'Co.'),\n",
       " ('Kwek', 'Hong', 'Png'),\n",
       " ('Kent', 'Neal'),\n",
       " ('Kitchen',),\n",
       " ('K-H', 'Corp.'),\n",
       " ('Kasler', 'Corp.'),\n",
       " ('Kimberly', 'Ann', 'Smith'),\n",
       " ('Kemper',),\n",
       " ('Keith', 'L.', 'Fogg'),\n",
       " ('Kathryn', 'McGrath'),\n",
       " ('Kobe', 'Steel', 'Ltd.')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_small_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolving the entities <a name='t6'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now implement a simple **method** to find the **named entities** from the previous exercise in **Wikipedia** and **Wikidata**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, look at a few entities in your set and find:\n",
    "1. a few entities that you think are in wikipedia, \n",
    "2. entities that will not be in wikipedia, and \n",
    "3. entities that you think are ambiguous: An entity that may correspond to two or more things. \n",
    "\n",
    "You will describe your findings in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to lookup entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the function below and try to understand what it means. You will describe it in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: Web Scraping \n",
    "When we **scrape the web**, we write code that sends a request to the server that’s hosting the page we specified: Wikipedia and Wikidata. Generally, our code downloads that page’s source code, just as a browser would. But instead of displaying the page visually, it filters through the page looking for **HTML elements** we’ve specified, and extracting whatever content we’ve instructed it to extract. using Python and the `Beautiful Soup` library is one of the most popular approaches to web scraping.\n",
    "\n",
    "When we visit a web page, our web browser makes a **request** to a web server. This request is called a `GET` request, since we’re getting files from the server. The server then sends back files that tell our browser how to render the page for us. When we perform web scraping, we’re interested in the **main content** of the web page, so we look at the HTML.\n",
    "\n",
    "The first thing we’ll need to do to scrape a web page is to download the page. We can download pages using the Python **requests library**. The requests library will make a `GET` request to a web server, which will download the HTML contents of a given web page for us.\n",
    "\n",
    "Why Web Scraping — When is this Needed?\n",
    "Web scraping is needed to unlock more powerful analysis when data isn’t available in an organized format. This could be useful for a variety of personal projects. You might, for example, want to scrape a sports website to analyze statistics associated with your favorite team.\n",
    "\n",
    "But web scraping can also be important for **data analysts** and **data scientists** in a business context. There’s an awful lot of data out on the web that simply isn’t available unless you scrape it (or painstakingly copy it into a spreadsheet by hand for analysis). When that data might contain valuable insights for your company or your industry, you’ll have to turn to web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_lookup(ner, base_url='https://en.wikipedia.org/wiki/'):\n",
    "    try:\n",
    "        # specify the website and named entities recognition ner ?\n",
    "        url_en = base_url + ' '.join(ner)\n",
    "        # get/download the webpage using Requests module\n",
    "        # then read the content of the server \n",
    "        html_doc = requests.get(url_en).text\n",
    "        \n",
    "        # web scraping using Beautiful Soup library\n",
    "        # Parsing the page \n",
    "        parse_tree = bs4.BeautifulSoup(html_doc, 'html.parser')\n",
    "        \n",
    "        # search for items using CSS selectors\n",
    "        # find the first instance of a tags \n",
    "        # a tags are links, and tell the browser to render a link to another web page. \n",
    "        # The href property of the tag determines where the link goes\n",
    "        entity_id = parse_tree.find(\"a\", {\"accesskey\": \"g\"})['href']\n",
    "        \n",
    "        head_id, entity_id = os.path.split(entity_id)\n",
    "        return entity_id\n",
    "    except:\n",
    "        pass\n",
    "        # print('Not found in: ', base_url)\n",
    "    entity_id = 'UNK'\n",
    "    return entity_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function to run the lookup and keep the **resolved entities** (only the resolved entities). You will call it `ne_ids_en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 76/76 [00:49<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "#Write your code here\n",
    "#from tqdm import tqdm\n",
    "ne_ids_en = []\n",
    "for ne in tqdm(ne_small_set):\n",
    "    ne_id = wikipedia_lookup(ne, base_url='https://en.wikipedia.org/wiki/')\n",
    "    ne_ids_en.append(ne_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q454315',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q493751',\n",
       " 'Q59268486',\n",
       " 'UNK',\n",
       " 'Q185582',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q6379829',\n",
       " 'Q21512656',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q225951',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q1730802',\n",
       " 'Q785671',\n",
       " 'Q1081154',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q486269',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q12223',\n",
       " 'UNK',\n",
       " 'Q1558',\n",
       " 'Q386682',\n",
       " 'Q95367',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q225951',\n",
       " 'Q327751',\n",
       " 'UNK',\n",
       " 'Q1603',\n",
       " 'Q745099',\n",
       " 'UNK',\n",
       " 'Q1865',\n",
       " 'Q3196386',\n",
       " 'Q2888777',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q358393',\n",
       " 'Q297659',\n",
       " 'UNK',\n",
       " 'Q6339344',\n",
       " 'Q13403399',\n",
       " 'UNK',\n",
       " 'Q224736',\n",
       " 'Q5838',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q181912',\n",
       " 'UNK',\n",
       " 'Q257417',\n",
       " 'UNK',\n",
       " 'Q10559663',\n",
       " 'UNK',\n",
       " 'Q43164',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q126993',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_ids_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, entities need a **confirmation**. You will apply the resolution with the **Swedish wikipedia**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 76/76 [00:44<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "ne_ids_sv = []\n",
    "for ne in tqdm(ne_small_set):\n",
    "    ne_id = wikipedia_lookup(ne, base_url='https://sv.wikipedia.org/wiki/')\n",
    "    ne_ids_sv.append(ne_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q232749',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q486269',\n",
       " 'UNK',\n",
       " 'Q153417',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q1558',\n",
       " 'Q386682',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q12857502',\n",
       " 'UNK',\n",
       " 'Q1603',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q1865',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q358393',\n",
       " 'Q297659',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q224736',\n",
       " 'Q5838',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q181912',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'Q952431',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'UNK']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_ids_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You will compute the **intersection** of the two sets. You will assign it to a **list** that you will **sort** and that you will call: `confirmed_ne_en_sv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "inter = [ne for ne in ne_ids_en if ne in ne_ids_sv and ne != 'UNK']\n",
    "#inter.sort()\n",
    "inter_inx = [ne_ids_en.index(ne) for ne in ne_ids_en if ne in ne_ids_sv and ne != 'UNK']\n",
    "confirmed_ne = [ne_small_set[i] for i in inter_inx]\n",
    "#confirmed_ne.sort()\n",
    "#confirmed_ne_en_sv = [(ne_small_set[i],inter[i]) for i in inter_inx]\n",
    "confirmed_ne_en_sv = list(zip(confirmed_ne,tuple(inter)))\n",
    "confirmed_ne_en_sv = sorted(confirmed_ne_en_sv, key=lambda ne: ne[0])\n",
    "#print(confirmed_ne_en_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q486269', 'Q1558', 'Q386682', 'Q1603', 'Q1865', 'Q358393', 'Q297659', 'Q224736', 'Q5838', 'Q181912']\n",
      "[25, 32, 33, 43, 46, 51, 52, 57, 58, 62]\n",
      "[('Kodak',), ('Kansas',), ('Khost',), ('Kentucky',), ('Kuala', 'Lumpur'), ('Kenton',), ('Kirin',), ('KIM',), ('Kabul',), ('KLM', 'Royal', 'Dutch', 'Airlines')]\n",
      "\n",
      " [(('KIM',), 'Q224736'), (('KLM', 'Royal', 'Dutch', 'Airlines'), 'Q181912'), (('Kabul',), 'Q5838'), (('Kansas',), 'Q1558'), (('Kenton',), 'Q358393'), (('Kentucky',), 'Q1603'), (('Khost',), 'Q386682'), (('Kirin',), 'Q297659'), (('Kodak',), 'Q486269'), (('Kuala', 'Lumpur'), 'Q1865')]\n"
     ]
    }
   ],
   "source": [
    "print(inter)\n",
    "print(inter_inx)\n",
    "print(confirmed_ne)\n",
    "print('\\n', confirmed_ne_en_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('KIM',), 'Q224736'),\n",
       " (('KLM', 'Royal', 'Dutch', 'Airlines'), 'Q181912'),\n",
       " (('Kabul',), 'Q5838'),\n",
       " (('Kansas',), 'Q1558'),\n",
       " (('Kenton',), 'Q358393'),\n",
       " (('Kentucky',), 'Q1603'),\n",
       " (('Khost',), 'Q386682'),\n",
       " (('Kirin',), 'Q297659'),\n",
       " (('Kodak',), 'Q486269'),\n",
       " (('Kuala', 'Lumpur'), 'Q1865')]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confirmed_ne_en_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **first items** in your list should look like:\n",
    "```\n",
    "[(('KIM',), 'Q224736'),\n",
    " (('KLM', 'Royal', 'Dutch', 'Airlines'), 'Q181912'),\n",
    " ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission <a name='t7'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have written all the code and run all the cells, fill in your ID and as well as the name of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "STIL_ID = [\"hi8826mo-s\"] # Write your stil ids as a list\n",
    "CURRENT_NOTEBOOK_PATH = os.path.join(os.getcwd(), \n",
    "                                     \"4-Chunker_HichamMohamad.ipynb\") # Write the name of your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission code will send your answer. It consists of the baseline score, the improved machine-learning score, and the confirmed entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"baseline_score\": 0.770671072299583, \"improved_ml_score\": 0.9231961786642086, \"confirmed_ne_en_sv\": [[[\"KIM\"], \"Q224736\"], [[\"KLM\", \"Royal\", \"Dutch\", \"Airlines\"], \"Q181912\"], [[\"Kabul\"], \"Q5838\"], [[\"Kansas\"], \"Q1558\"], [[\"Kenton\"], \"Q358393\"], [[\"Kentucky\"], \"Q1603\"], [[\"Khost\"], \"Q386682\"], [[\"Kirin\"], \"Q297659\"], [[\"Kodak\"], \"Q486269\"], [[\"Kuala\", \"Lumpur\"], \"Q1865\"]]}'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "ANSWER = json.dumps({'baseline_score': baseline_score,\n",
    "                    'improved_ml_score': improved_ml_score,\n",
    "                    'confirmed_ne_en_sv': confirmed_ne_en_sv})\n",
    "ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the moment of truth:\n",
    "1. Save your notebook and\n",
    "2. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_NOTEBOOK_PATH = CURRENT_NOTEBOOK_PATH + \".submission.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "ASSIGNMENT = 4\n",
    "API_KEY = \"f581ba347babfea0b8f2c74a3a6776a7\"\n",
    "\n",
    "# Copy and compress current notebook\n",
    "with bz2.open(SUBMISSION_NOTEBOOK_PATH, mode=\"wb\") as fout:\n",
    "    with open(CURRENT_NOTEBOOK_PATH, \"rb\") as fin:\n",
    "        fout.write(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'msg': None,\n",
       " 'status': 'correct',\n",
       " 'signature': 'fbe032d32551fb78f8d8ad3808f9f349cc84d400a572246141e6c13a24168ddd778bc15629f4642ea65aab153385a07e11a557e3f403171f59916570280f8df9',\n",
       " 'submission_id': 'ff442bcc-eb41-492e-ad81-0b3e75be3de9'}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.post(\"https://vilde.cs.lth.se/edan20checker/submit\", \n",
    "                    files={\"notebook_file\": open(SUBMISSION_NOTEBOOK_PATH, \"rb\")}, \n",
    "                    data={\n",
    "                        \"stil_id\": STIL_ID,\n",
    "                        \"assignment\": ASSIGNMENT,\n",
    "                        \"answer\": ANSWER,\n",
    "                        \"api_key\": API_KEY,\n",
    "                    },\n",
    "               verify=True)\n",
    "\n",
    "# from IPython.display import display, JSON\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading <a name='t8'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will read the article, <a href=\"https://www.aclweb.org/anthology/C18-1139\"><i>Contextual String\n",
    "            Embeddings for Sequence Labeling</i></a> by Akbik et al. (2018)\n",
    "            and you will outline the **main differences** between their system and yours. A LSTM is a type of\n",
    "            **recurrent neural network**, while CRF is a sort of **beam search**. You will tell the performance\n",
    "            they reach on the corpus you used in this laboratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
