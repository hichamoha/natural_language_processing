{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #1: Building an inverted index\n",
    "\n",
    "Author: **Hicham Mohamad** (hi8826mo-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Natural Language Processing** or **NLP** is a field of Artificial Intelligence that gives the machines the ability to read, understand and derive meaning from human languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Corpus](#t1)\n",
    "2. [Indexing one file](#t2)\n",
    "3. [Reading the content of a folder](#t3)\n",
    "4. [Creating a master index](#t4)\n",
    "5. [Representing Documents with tf-idf](#t5)\n",
    "6. [Comparing Documents](#t6)\n",
    "7. [Submission](#t7)\n",
    "8. [Reading](#t8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objectives of this assignment are to:\n",
    "* Write a program that collects all the words from a set of documents\n",
    "* Build an index from the words\n",
    "* Represent a document using the Tf.Idf values\n",
    "* Write a short report of 1 to 2 pages on the assignment\n",
    "* Read a short text on an industrial system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have written all the missing code and run all the cells, *you will submit your notebook to an automatic marking system*. Do not erase the content of the cells as we will possibly check your programs manually.\n",
    "The submission instructions are at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will **build an indexer to index all the words in a corpus**. Conceptually, an **index** consists of rows with one word per row and the **list** of files and positions, where this word occurs. Such a row is called a _posting list_. You will encode the position of a word by the number of characters from the start of the file.\n",
    "<pre>\n",
    "word1: file_name pos1 pos2 pos3... file_name pos1 pos2 ...\n",
    "word2: file_name pos1 pos2 pos3... file_name pos1 pos2 ...\n",
    "...\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some imports. Add others as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import regex as re\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus <a name=\"t1\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will create an index for a corpus of **Selma Lagerlöf's works**: To gather the corpus, you can alternatively:\n",
    "1. Download the <a href=\"https://github.com/pnugues/ilppp/raw/master/programs/corpus/Selma.zip\">Selma folder</a> and uncompress it. It contains novels by <a href=\"https://sv.wikipedia.org/wiki/Selma_Lagerl%C3%B6f\">Selma Lagerlöf</a>. The text of these novels was extracted from <a href=\"https://litteraturbanken.se/forfattare/LagerlofS/titlar\">Lagerlöf arkivet</a> at <a href=\"https://litteraturbanken.se/\">Litteraturbanken</a>.\n",
    "2. Or run this cell that will download the corpus and place it in your folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selma has been downloaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Selma\\\\bannlyst.txt',\n",
       " 'Selma\\\\gosta.txt',\n",
       " 'Selma\\\\herrgard.txt',\n",
       " 'Selma\\\\jerusalem.txt',\n",
       " 'Selma\\\\kejsaren.txt',\n",
       " 'Selma\\\\marbacka.txt',\n",
       " 'Selma\\\\nils.txt',\n",
       " 'Selma\\\\osynliga.txt',\n",
       " 'Selma\\\\troll.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for Selma dataset\n",
    "SELMA_URL = \"https://github.com/pnugues/ilppp/raw/master/programs/corpus/Selma.zip\"\n",
    "\n",
    "SELMA_FILES = [\n",
    "    os.path.join(\"Selma\", fname) \n",
    "    for fname in \n",
    "    [\n",
    "        \"bannlyst.txt\", \n",
    "        \"gosta.txt\", \n",
    "        \"herrgard.txt\", \n",
    "        \"jerusalem.txt\", \n",
    "        \"kejsaren.txt\", \n",
    "        \"marbacka.txt\", \n",
    "        \"nils.txt\", \n",
    "        \"osynliga.txt\", \n",
    "        \"troll.txt\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "def download_and_extract_selma():\n",
    "    \"\"\"Downloads and unpacks Selma.zip\"\"\"\n",
    "    \n",
    "    global SELMA_URL\n",
    "    # Download if not all files exist\n",
    "    req = requests.get(SELMA_URL, stream=True)\n",
    "    if req.status_code != 200:\n",
    "        print(\"Failed to download file, got status: \" + req.status_code)\n",
    "        req.close()\n",
    "    else:\n",
    "        with open(\"Selma.zip\", \"wb\") as fd:\n",
    "            written = 0\n",
    "            for chunk in req.iter_content(chunk_size=65536):\n",
    "                fd.write(chunk)\n",
    "                written += len(chunk)\n",
    "                print(\"Downloading: %d bytes written to Selma.zip\" % written)\n",
    "\n",
    "        print(\"Selma.zip donwnloaded.\")\n",
    "        req.close()\n",
    "        \n",
    "        selma_zipfile = ZipFile(\"Selma.zip\")\n",
    "        selma_files_to_extract = [zi for zi in selma_zipfile.filelist if not zi.filename.startswith(\"__\") and zi.filename.endswith(\".txt\")]\n",
    "        for zi in selma_files_to_extract:\n",
    "            selma_zipfile.extract(zi)\n",
    "            print(\"Extracted: \" + zi.filename)\n",
    "            \n",
    "        print(\"Done!\")\n",
    "        \n",
    "# If not all path exists (all are true), then download\n",
    "if not all([os.path.exists(fname) for fname in SELMA_FILES]):\n",
    "    download_and_extract_selma()\n",
    "else:\n",
    "    print(\"Selma has been downloaded.\")\n",
    "    \n",
    "SELMA_FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the indexer (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a production context, your final program would take a corpus as input (here the Selma Lagerlöf's novels) and create an index of all the words with their positions. You should be able to run it this way:\n",
    "<pre>$ python indexer.py folder_name</pre>\n",
    "In this lab, you will write the index in a Jupyter Notebook. The conversion into a Python program is left as an optional exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming the Indexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make programming easier, you will split this exercise into five steps:\n",
    "1. Index one **file**\n",
    "2. Read the **content** of a folder\n",
    "3. Create a **master index** for all the files\n",
    "4. Use **tfidf** to represent the documents (novels)\n",
    "5. Compare the documents of a collection\n",
    "\n",
    "You will use **dictionaries** to represent the **postings**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing one file <a name=\"t2\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Write a program that reads one document <tt>file_name.txt</tt> and outputs an index file:\n",
    "            <tt>file_name.idx</tt>:\n",
    "        </p>\n",
    "        <ol>\n",
    "    <li>The index file will contain all the <b>unique words</b> in the document,\n",
    "        where each word is associated with the list of its <b>positions</b> in the document.\n",
    "            </li>\n",
    "    <li>You will represent this index as a <b>dictionary</b>, where the keys will be the words, and\n",
    "                the values, the lists of positions\n",
    "            </li>\n",
    "    <li>As <b>words</b>, you will consider all the strings of letters that you will set in lower case.\n",
    "                You will not index the rest (i.e. numbers, punctuations, or symbols).\n",
    "            </li>\n",
    "            <li>To extract the words, use <b>Unicode regular expressions</b>. Do not use <tt>\\w+</tt>,\n",
    "                for instance, but the Unicode equivalent.\n",
    "            </li>\n",
    "            <li>The <b>word positions</b> will correspond to the number of characters from the beginning of the file.\n",
    "                (The word offset from the beginning)\n",
    "            </li>\n",
    "            <li>You will use the <tt>finditer()</tt> method to find the positions of the words.\n",
    "                This will return you <b>match objects</b>,\n",
    "                where you will get the matches and the positions with\n",
    "                the <tt>group()</tt> and <tt>start()</tt> methods.\n",
    "            </li>\n",
    "    <li>You will use the <b>pickle package</b> to write your dictionary in an file,\n",
    "                see <a href=\"https://wiki.python.org/moin/UsingPickle\">https://wiki.python.org/moin/UsingPickle</a>.\n",
    "            </li>\n",
    "        </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an excerpt of the index of the `bannlyst.txt` text for the words <i>gjord</i>, <i>uppklarnande</i>, and <i>stjärnor</i>. The data is stored in a dictionary:\n",
    "\n",
    "<pre>\n",
    "{...\n",
    "'gjord': [8600, 183039, 220445],\n",
    "'uppklarnande': [8617],\n",
    "'stjärnor': [8641], ...\n",
    "}\n",
    "</pre>\n",
    "where the word <i>gjord</i> occurs three times in the text at positions 8600, 183039, and 220445, <i>uppklarnande</i>, once at position 8617, and <i>stjärnor</i>, once at position 8641."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a tokenizer \n",
    " Tokenizing texts using white spaces as word delimiters is the most elementary technique. we just replace sequences of white spaces in the text with a new line, and we consider what is between two white spaces to be a word. In the program, we use the \\s character class\n",
    "to represent the **white space**:\n",
    "<pre>\n",
    "import re \n",
    "one_token_per_line = re.sub(’\\s+’, ’\\n’, text)\n",
    "</pre>\n",
    "\n",
    "However, this does not work perfectly where the commas are not segmented from the words.\n",
    "So it failed to tokenize the punctuation. To improve it, we need to process separately the punctuation and symbols. *We identify these characters with regular expressions and we insert white spaces around them to separate them from the words*. \n",
    "\n",
    "It is easier and more compact to use\n",
    "the Unicode **punctuation** and **symbol** classes instead: \\p{P} and \\p{S}, respectively. To use the Unicode classes, we need to import the regex module.\n",
    "We then tokenize the text according to white spaces as in the previous section:\n",
    "<pre>\n",
    "import regex as re\n",
    "spaced_tokens = re.sub(’([\\p{S}\\p{P}])’, r’ \\1 ’, text)\n",
    "one_token_per_line = re.sub(’\\s+’, ’\\n’, spaced_tokens)\n",
    "</pre>\n",
    "Alternatively, as shown below, we can explicitely define the **content of words**. we can extract a sequence of words, where a word is defined as a sequence of contiguous letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a Unicode regular expression to find words defined as **sequences of letters**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your regex here\n",
    "\n",
    "# Using the content, a word is defined as a sequence of contiguous letters\n",
    "letters_regex = '\\p{L}+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['En',\n",
       " 'gång',\n",
       " 'hade',\n",
       " 'de',\n",
       " 'på',\n",
       " 'Mårbacka',\n",
       " 'en',\n",
       " 'barnpiga',\n",
       " 'som',\n",
       " 'hette',\n",
       " 'Back',\n",
       " 'Kajsa']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(letters_regex, 'En gång hade de på Mårbacka en barnpiga, som hette Back-Kajsa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `regex`, write `tokenize(text)` function to tokenize a text. Return their positions. \n",
    "\n",
    "We extract the words with the tokenizer as before but instead of `findall()`, we use `finditer()` to return the **match objects**. We use these match objects to extract the word positions. (See Book p 170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\"\"\"\n",
    "Uses the letters to break the text into words.\n",
    "Returns a list of match objects.\n",
    "\"\"\"\n",
    "def tokenize(text):\n",
    "    wordTokens = re.finditer('\\p{L}+', text)\n",
    "    return wordTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<regex.Match object; span=(0, 2), match='En'>,\n",
       " <regex.Match object; span=(3, 7), match='gång'>,\n",
       " <regex.Match object; span=(8, 12), match='hade'>,\n",
       " <regex.Match object; span=(13, 15), match='de'>,\n",
       " <regex.Match object; span=(16, 18), match='på'>,\n",
       " <regex.Match object; span=(19, 27), match='Mårbacka'>,\n",
       " <regex.Match object; span=(28, 30), match='en'>,\n",
       " <regex.Match object; span=(31, 39), match='barnpiga'>,\n",
       " <regex.Match object; span=(41, 44), match='som'>,\n",
       " <regex.Match object; span=(45, 50), match='hette'>,\n",
       " <regex.Match object; span=(51, 55), match='Back'>,\n",
       " <regex.Match object; span=(56, 61), match='Kajsa'>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize('En gång hade de på Mårbacka en barnpiga, som hette Back-Kajsa.')\n",
    "list(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a `text_to_idx(words)` function to extract the indices from the list of tokens (words). Return a **dictionary**, where the keys will be the tokens (words), and the values a list of positions.\n",
    "\n",
    "*This will return us match objects, where we get the matches and the positions with the group() and start() methods.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\"\"\"\n",
    "Builds an index from a list of match objects.\n",
    "\"\"\"\n",
    "def text_to_idx(words):\n",
    "    word_idx = {}\n",
    "    # get the matches and the positions with the group() and start() methods\n",
    "    for word in words:\n",
    "        try:\n",
    "            word_idx[word.group()].append(word.start())\n",
    "        except:\n",
    "            word_idx[word.group()] = [word.start()]\n",
    "    return word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': [0, 28],\n",
       " 'gång': [3],\n",
       " 'hade': [8],\n",
       " 'de': [13],\n",
       " 'på': [16],\n",
       " 'mårbacka': [19],\n",
       " 'barnpiga': [31],\n",
       " 'som': [41],\n",
       " 'hette': [45],\n",
       " 'back': [51],\n",
       " 'kajsa': [56]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize('En gång hade de på Mårbacka en barnpiga, som hette Back-Kajsa.'.lower().strip())\n",
    "text_to_idx(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading one file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read one file, _Mårbacka_, `marbacka.txt`, set it in lowercase, tokenize it, and index it. Call this index `idx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "#marbackafile = open(\"Selma/marbacka.txt\", \"r\", encoding=\"utf8\")\n",
    "marbackafile = open(\"Selma\\marbacka.txt\", encoding='utf-8')\n",
    "marbackatxt = marbackafile.read()\n",
    "#marbackafile.close()\n",
    "\n",
    "marbaTokens = tokenize(marbackatxt.lower().strip())\n",
    "#marbaTokens = tokenize(marbackatxt.lower())\n",
    "idx = text_to_idx(marbaTokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16,\n",
       " 139,\n",
       " 752,\n",
       " 1700,\n",
       " 2582,\n",
       " 3324,\n",
       " 15117,\n",
       " 15404,\n",
       " 27794,\n",
       " 42175,\n",
       " 49126,\n",
       " 50407,\n",
       " 52053,\n",
       " 60144,\n",
       " 63374,\n",
       " 64910,\n",
       " 67182,\n",
       " 67330,\n",
       " 67799,\n",
       " 67824,\n",
       " 69232,\n",
       " 71328,\n",
       " 72099,\n",
       " 74147,\n",
       " 74255,\n",
       " 74614,\n",
       " 76610,\n",
       " 76884,\n",
       " 77138,\n",
       " 77509,\n",
       " 77787,\n",
       " 77936,\n",
       " 78574,\n",
       " 80597,\n",
       " 81782,\n",
       " 82003,\n",
       " 84363,\n",
       " 84786,\n",
       " 85251,\n",
       " 89837,\n",
       " 97093,\n",
       " 98642,\n",
       " 100474,\n",
       " 105063,\n",
       " 105298,\n",
       " 105721,\n",
       " 108710,\n",
       " 109133,\n",
       " 112844,\n",
       " 113725,\n",
       " 114997,\n",
       " 115583,\n",
       " 115833,\n",
       " 116368,\n",
       " 116557,\n",
       " 121896,\n",
       " 124823,\n",
       " 126409,\n",
       " 126542,\n",
       " 128758,\n",
       " 130976,\n",
       " 131939,\n",
       " 132826,\n",
       " 136914,\n",
       " 137187,\n",
       " 137872,\n",
       " 139196,\n",
       " 140721,\n",
       " 142324,\n",
       " 146781,\n",
       " 151497,\n",
       " 154335,\n",
       " 155139,\n",
       " 155438,\n",
       " 155886,\n",
       " 156405,\n",
       " 158108,\n",
       " 159817,\n",
       " 160107,\n",
       " 161158,\n",
       " 162085,\n",
       " 165847,\n",
       " 168316,\n",
       " 168528,\n",
       " 169111,\n",
       " 170333,\n",
       " 172684,\n",
       " 182047,\n",
       " 182427,\n",
       " 186362,\n",
       " 189535,\n",
       " 190999,\n",
       " 191110,\n",
       " 193177,\n",
       " 196686,\n",
       " 202552,\n",
       " 206340,\n",
       " 207789,\n",
       " 208382,\n",
       " 209874,\n",
       " 210525,\n",
       " 217464,\n",
       " 219933,\n",
       " 221393,\n",
       " 221533,\n",
       " 221880,\n",
       " 222213,\n",
       " 224190,\n",
       " 229501,\n",
       " 229598,\n",
       " 230783,\n",
       " 231453,\n",
       " 232140,\n",
       " 234427,\n",
       " 236193,\n",
       " 236950,\n",
       " 240168,\n",
       " 241891,\n",
       " 242359,\n",
       " 242934,\n",
       " 243030,\n",
       " 244831,\n",
       " 249882,\n",
       " 251277,\n",
       " 251901,\n",
       " 256360,\n",
       " 260244,\n",
       " 261612,\n",
       " 262384,\n",
       " 263856,\n",
       " 266638,\n",
       " 269760,\n",
       " 270113,\n",
       " 270674,\n",
       " 271146,\n",
       " 271885,\n",
       " 272560,\n",
       " 273464,\n",
       " 275086,\n",
       " 275664,\n",
       " 276207,\n",
       " 277407,\n",
       " 292648,\n",
       " 299762,\n",
       " 306277,\n",
       " 307507,\n",
       " 307972,\n",
       " 308148,\n",
       " 308330,\n",
       " 308568,\n",
       " 311856,\n",
       " 317491,\n",
       " 321194,\n",
       " 321925,\n",
       " 328154,\n",
       " 328470,\n",
       " 328977,\n",
       " 330435,\n",
       " 331650,\n",
       " 337494,\n",
       " 340526,\n",
       " 348636,\n",
       " 349331,\n",
       " 350022,\n",
       " 350168,\n",
       " 350674,\n",
       " 350949,\n",
       " 351349,\n",
       " 354175,\n",
       " 354411,\n",
       " 356314,\n",
       " 356541,\n",
       " 356788,\n",
       " 357522,\n",
       " 358413,\n",
       " 359478,\n",
       " 360511,\n",
       " 362108,\n",
       " 363675,\n",
       " 364467,\n",
       " 365049,\n",
       " 366954,\n",
       " 367286,\n",
       " 367741,\n",
       " 368878,\n",
       " 369487,\n",
       " 373757,\n",
       " 377123,\n",
       " 378852,\n",
       " 379696]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx['mårbacka']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save your index in a file so that you can reuse it. Use the **pickle module**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "pickle.dump(idx, open(\"savedMarbackaIdx.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "savedidx = pickle.load(open(\"savedMarbackaIdx.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16,\n",
       " 139,\n",
       " 752,\n",
       " 1700,\n",
       " 2582,\n",
       " 3324,\n",
       " 15117,\n",
       " 15404,\n",
       " 27794,\n",
       " 42175,\n",
       " 49126,\n",
       " 50407,\n",
       " 52053,\n",
       " 60144,\n",
       " 63374,\n",
       " 64910,\n",
       " 67182,\n",
       " 67330,\n",
       " 67799,\n",
       " 67824,\n",
       " 69232,\n",
       " 71328,\n",
       " 72099,\n",
       " 74147,\n",
       " 74255,\n",
       " 74614,\n",
       " 76610,\n",
       " 76884,\n",
       " 77138,\n",
       " 77509,\n",
       " 77787,\n",
       " 77936,\n",
       " 78574,\n",
       " 80597,\n",
       " 81782,\n",
       " 82003,\n",
       " 84363,\n",
       " 84786,\n",
       " 85251,\n",
       " 89837,\n",
       " 97093,\n",
       " 98642,\n",
       " 100474,\n",
       " 105063,\n",
       " 105298,\n",
       " 105721,\n",
       " 108710,\n",
       " 109133,\n",
       " 112844,\n",
       " 113725,\n",
       " 114997,\n",
       " 115583,\n",
       " 115833,\n",
       " 116368,\n",
       " 116557,\n",
       " 121896,\n",
       " 124823,\n",
       " 126409,\n",
       " 126542,\n",
       " 128758,\n",
       " 130976,\n",
       " 131939,\n",
       " 132826,\n",
       " 136914,\n",
       " 137187,\n",
       " 137872,\n",
       " 139196,\n",
       " 140721,\n",
       " 142324,\n",
       " 146781,\n",
       " 151497,\n",
       " 154335,\n",
       " 155139,\n",
       " 155438,\n",
       " 155886,\n",
       " 156405,\n",
       " 158108,\n",
       " 159817,\n",
       " 160107,\n",
       " 161158,\n",
       " 162085,\n",
       " 165847,\n",
       " 168316,\n",
       " 168528,\n",
       " 169111,\n",
       " 170333,\n",
       " 172684,\n",
       " 182047,\n",
       " 182427,\n",
       " 186362,\n",
       " 189535,\n",
       " 190999,\n",
       " 191110,\n",
       " 193177,\n",
       " 196686,\n",
       " 202552,\n",
       " 206340,\n",
       " 207789,\n",
       " 208382,\n",
       " 209874,\n",
       " 210525,\n",
       " 217464,\n",
       " 219933,\n",
       " 221393,\n",
       " 221533,\n",
       " 221880,\n",
       " 222213,\n",
       " 224190,\n",
       " 229501,\n",
       " 229598,\n",
       " 230783,\n",
       " 231453,\n",
       " 232140,\n",
       " 234427,\n",
       " 236193,\n",
       " 236950,\n",
       " 240168,\n",
       " 241891,\n",
       " 242359,\n",
       " 242934,\n",
       " 243030,\n",
       " 244831,\n",
       " 249882,\n",
       " 251277,\n",
       " 251901,\n",
       " 256360,\n",
       " 260244,\n",
       " 261612,\n",
       " 262384,\n",
       " 263856,\n",
       " 266638,\n",
       " 269760,\n",
       " 270113,\n",
       " 270674,\n",
       " 271146,\n",
       " 271885,\n",
       " 272560,\n",
       " 273464,\n",
       " 275086,\n",
       " 275664,\n",
       " 276207,\n",
       " 277407,\n",
       " 292648,\n",
       " 299762,\n",
       " 306277,\n",
       " 307507,\n",
       " 307972,\n",
       " 308148,\n",
       " 308330,\n",
       " 308568,\n",
       " 311856,\n",
       " 317491,\n",
       " 321194,\n",
       " 321925,\n",
       " 328154,\n",
       " 328470,\n",
       " 328977,\n",
       " 330435,\n",
       " 331650,\n",
       " 337494,\n",
       " 340526,\n",
       " 348636,\n",
       " 349331,\n",
       " 350022,\n",
       " 350168,\n",
       " 350674,\n",
       " 350949,\n",
       " 351349,\n",
       " 354175,\n",
       " 354411,\n",
       " 356314,\n",
       " 356541,\n",
       " 356788,\n",
       " 357522,\n",
       " 358413,\n",
       " 359478,\n",
       " 360511,\n",
       " 362108,\n",
       " 363675,\n",
       " 364467,\n",
       " 365049,\n",
       " 366954,\n",
       " 367286,\n",
       " 367741,\n",
       " 368878,\n",
       " 369487,\n",
       " 373757,\n",
       " 377123,\n",
       " 378852,\n",
       " 379696]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savedidx['mårbacka']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the content of a folder <a name= \"t3\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `get_files(dir, suffix)` function that reads all the files in a folder with a specific `suffix` (txt). You will need the Python `os` package, see <a href=\"https://docs.python.org/3/library/os.html\">https://docs.python.org/3/library/os.html</a>. You will return the file names in a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reuse this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dir, suffix):\n",
    "    \"\"\"\n",
    "    Returns all the files in a folder ending with suffix\n",
    "    :param dir:\n",
    "    :param suffix:\n",
    "    :return: the list of file names\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith(suffix):\n",
    "            files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bannlyst.txt',\n",
       " 'gosta.txt',\n",
       " 'herrgard.txt',\n",
       " 'jerusalem.txt',\n",
       " 'kejsaren.txt',\n",
       " 'marbacka.txt',\n",
       " 'nils.txt',\n",
       " 'osynliga.txt',\n",
       " 'troll.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "SelmaFiles = get_files('Selma', '.txt')\n",
    "SelmaFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a master index <a name=\"t4\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete your program with the creation of **master index**, where you will associate each word of the corpus with the files, where it occur and its positions: a posting list.\n",
    "\n",
    "Below is an excerpt of the master index with the words *samlar* and *ände*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'samlar': {'troll.txt': [641880, 654233],\n",
       "  'nils.txt': [51805, 118943],\n",
       "  'osynliga.txt': [399121],\n",
       "  'gosta.txt': [313784, 409998, 538165]},\n",
       " 'ände': {'troll.txt': [39562, 650112],\n",
       "  'kejsaren.txt': [50171],\n",
       "  'marbacka.txt': [370324],\n",
       "  'nils.txt': [1794],\n",
       "  'osynliga.txt': [272144]}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'samlar':\n",
    "            {'troll.txt': [641880, 654233],\n",
    "            'nils.txt': [51805, 118943],\n",
    "            'osynliga.txt': [399121],\n",
    "            'gosta.txt': [313784, 409998, 538165]},\n",
    " 'ände':\n",
    "            {'troll.txt': [39562, 650112],\n",
    "            'kejsaren.txt': [50171],\n",
    "            'marbacka.txt': [370324],\n",
    "            'nils.txt': [1794],\n",
    "            'osynliga.txt': [272144]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word <i>samlar</i>, for instance, occurs three times in the gosta text at positions\n",
    "            313784, 409998, and 538165."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the master index of corpus Selma with a loop over the list of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "\n",
    "corpus_files = get_files('Selma', '.txt')\n",
    "#print(\"Selma Files: \", len(corpus_files), \"\\n\", corpus_files)\n",
    "\n",
    "master_index = {}\n",
    "\n",
    "for file in corpus_files:\n",
    "    filetext = open('Selma/' + file, encoding = 'utf-8').read()\n",
    "    \n",
    "    # list of tokens (words): match objects\n",
    "    filewords = tokenize(filetext.lower().strip())\n",
    "    \n",
    "    # text_to_idx Returns a dictionary of words|positions\n",
    "    # we get the matches and the positions with the group() and start() methods.\n",
    "    file_idx = text_to_idx(filewords)\n",
    "        \n",
    "    for word in file_idx:\n",
    "        if word in master_index:\n",
    "            master_index[word][file] = file_idx[word]\n",
    "            \n",
    "        else:\n",
    "            master_index[word] = {}\n",
    "            master_index[word][file] = file_idx[word]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kejsaren.txt': [50171],\n",
       " 'marbacka.txt': [370324],\n",
       " 'nils.txt': [1794],\n",
       " 'osynliga.txt': [272144],\n",
       " 'troll.txt': [39562, 650112]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_index['ände']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gosta.txt': [313784, 409998, 538165],\n",
       " 'nils.txt': [51805, 118943],\n",
       " 'osynliga.txt': [399121],\n",
       " 'troll.txt': [641880, 654233]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_index['samlar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'marbacka.txt': [16,\n",
       "  139,\n",
       "  752,\n",
       "  1700,\n",
       "  2582,\n",
       "  3324,\n",
       "  15117,\n",
       "  15404,\n",
       "  27794,\n",
       "  42175,\n",
       "  49126,\n",
       "  50407,\n",
       "  52053,\n",
       "  60144,\n",
       "  63374,\n",
       "  64910,\n",
       "  67182,\n",
       "  67330,\n",
       "  67799,\n",
       "  67824,\n",
       "  69232,\n",
       "  71328,\n",
       "  72099,\n",
       "  74147,\n",
       "  74255,\n",
       "  74614,\n",
       "  76610,\n",
       "  76884,\n",
       "  77138,\n",
       "  77509,\n",
       "  77787,\n",
       "  77936,\n",
       "  78574,\n",
       "  80597,\n",
       "  81782,\n",
       "  82003,\n",
       "  84363,\n",
       "  84786,\n",
       "  85251,\n",
       "  89837,\n",
       "  97093,\n",
       "  98642,\n",
       "  100474,\n",
       "  105063,\n",
       "  105298,\n",
       "  105721,\n",
       "  108710,\n",
       "  109133,\n",
       "  112844,\n",
       "  113725,\n",
       "  114997,\n",
       "  115583,\n",
       "  115833,\n",
       "  116368,\n",
       "  116557,\n",
       "  121896,\n",
       "  124823,\n",
       "  126409,\n",
       "  126542,\n",
       "  128758,\n",
       "  130976,\n",
       "  131939,\n",
       "  132826,\n",
       "  136914,\n",
       "  137187,\n",
       "  137872,\n",
       "  139196,\n",
       "  140721,\n",
       "  142324,\n",
       "  146781,\n",
       "  151497,\n",
       "  154335,\n",
       "  155139,\n",
       "  155438,\n",
       "  155886,\n",
       "  156405,\n",
       "  158108,\n",
       "  159817,\n",
       "  160107,\n",
       "  161158,\n",
       "  162085,\n",
       "  165847,\n",
       "  168316,\n",
       "  168528,\n",
       "  169111,\n",
       "  170333,\n",
       "  172684,\n",
       "  182047,\n",
       "  182427,\n",
       "  186362,\n",
       "  189535,\n",
       "  190999,\n",
       "  191110,\n",
       "  193177,\n",
       "  196686,\n",
       "  202552,\n",
       "  206340,\n",
       "  207789,\n",
       "  208382,\n",
       "  209874,\n",
       "  210525,\n",
       "  217464,\n",
       "  219933,\n",
       "  221393,\n",
       "  221533,\n",
       "  221880,\n",
       "  222213,\n",
       "  224190,\n",
       "  229501,\n",
       "  229598,\n",
       "  230783,\n",
       "  231453,\n",
       "  232140,\n",
       "  234427,\n",
       "  236193,\n",
       "  236950,\n",
       "  240168,\n",
       "  241891,\n",
       "  242359,\n",
       "  242934,\n",
       "  243030,\n",
       "  244831,\n",
       "  249882,\n",
       "  251277,\n",
       "  251901,\n",
       "  256360,\n",
       "  260244,\n",
       "  261612,\n",
       "  262384,\n",
       "  263856,\n",
       "  266638,\n",
       "  269760,\n",
       "  270113,\n",
       "  270674,\n",
       "  271146,\n",
       "  271885,\n",
       "  272560,\n",
       "  273464,\n",
       "  275086,\n",
       "  275664,\n",
       "  276207,\n",
       "  277407,\n",
       "  292648,\n",
       "  299762,\n",
       "  306277,\n",
       "  307507,\n",
       "  307972,\n",
       "  308148,\n",
       "  308330,\n",
       "  308568,\n",
       "  311856,\n",
       "  317491,\n",
       "  321194,\n",
       "  321925,\n",
       "  328154,\n",
       "  328470,\n",
       "  328977,\n",
       "  330435,\n",
       "  331650,\n",
       "  337494,\n",
       "  340526,\n",
       "  348636,\n",
       "  349331,\n",
       "  350022,\n",
       "  350168,\n",
       "  350674,\n",
       "  350949,\n",
       "  351349,\n",
       "  354175,\n",
       "  354411,\n",
       "  356314,\n",
       "  356541,\n",
       "  356788,\n",
       "  357522,\n",
       "  358413,\n",
       "  359478,\n",
       "  360511,\n",
       "  362108,\n",
       "  363675,\n",
       "  364467,\n",
       "  365049,\n",
       "  366954,\n",
       "  367286,\n",
       "  367741,\n",
       "  368878,\n",
       "  369487,\n",
       "  373757,\n",
       "  377123,\n",
       "  378852,\n",
       "  379696],\n",
       " 'nils.txt': [991703],\n",
       " 'troll.txt': [226291, 387634, 392959]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_index['mårbacka']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save your master index in a file and read it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# save the master index in a file using pickle package\n",
    "pickle.dump(master_index, open(\"savedMasterIdx.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read again the saved master index\n",
    "saved_master_index = pickle.load(open(\"savedMasterIdx.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gosta.txt': [313784, 409998, 538165],\n",
       " 'nils.txt': [51805, 118943],\n",
       " 'osynliga.txt': [399121],\n",
       " 'troll.txt': [641880, 654233]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_master_index['samlar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concordances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a `concordance(word, master_index, window)` function to extract the concordances of a `word` within a `window` of characters.\n",
    "\n",
    "Concordances are used to read **occurrences** of the given terms (or expression) in their respective context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def concordance(word, master_index, window):\n",
    "    concord = {}\n",
    "    if word in master_index:\n",
    "        \n",
    "        for filename in master_index[word]:\n",
    "            #print(filename)\n",
    "            try:\n",
    "                filetext = open('Selma/' + filename, encoding='utf-8').read()\n",
    "                concord[filename] = \"\"\n",
    "            except:\n",
    "                print('Could not open file', filename)\n",
    "            \n",
    "            for i in master_index[word][filename]:\n",
    "            #for i in master_index[word]:\n",
    "                if filename in concord:\n",
    "                    concord[filename] += \"\\n\\t\" + filetext[(i-window):(i+window)].replace(\"\\n\", ' ')\n",
    "                else:\n",
    "                    concord[filename] = []\n",
    "                    concord.append(filetext[(i-window):(i+window)].replace(\"\\n\", ' '))\n",
    "                \n",
    "        for file in concord:\n",
    "                print(file, concord.get(file))\n",
    "        \n",
    "                       \n",
    "            # spaces match tabs and newlines\n",
    "#            pattern = re.sub(' ', '\\\\s+', word)\n",
    "            # Replaces newline with spaces in the text\n",
    "#            text = re.sub('\\s+', ' ', filetext)\n",
    "            \n",
    "#            concord = \"(.{{0,{width}}}{pattern}.{{0,{width}}})\".format(pattern=pattern, width=window)\n",
    "            #print(i.group(1))\n",
    "#            for match in re.finditer(concord, text):\n",
    "#                print(\"\\t\", match.group(1))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gosta.txt \n",
      "\tom ligger nära Borg, och samlar ihop ett litet mid\n",
      "\tlika förstämda.  Men hon samlar upp allt detta som\n",
      "\tn ensam i livet.  Därmed samlar han korten tillhop\n",
      "nils.txt \n",
      "\t bara, att du i all hast samlar ihop så mycket bos\n",
      "\tar stannat hemma, och nu samlar de sig för att int\n",
      "osynliga.txt \n",
      "\t till höger i kärran och samlar just ihop tömmarna\n",
      "troll.txt \n",
      "\ten örtkunnig läkare, som samlar in markens växter \n",
      "\tälper dem, och medan hon samlar och handlar för de\n"
     ]
    }
   ],
   "source": [
    "concordance('samlar', master_index, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Documents with tf-idf <a name=\"t5\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created the index, you will represent each document in your corpus as a **dictionary**. The **keys** of these dictionaries will be the words and you will define the **value** of a word with the tf-idf metric: \n",
    "1. Read the description of the tf-idf measure on Wikipedia (<a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\">https://en.wikipedia.org/wiki/Tf-idf</a>)\n",
    "2. After reading the description, you probably realized that there are multiple definitions of tf-idf. In this assignment, \n",
    " * Tf will be the **relative frequency** of the term in the document and \n",
    " * idf, the **logarithm base 10** of the inverse document frequency.\n",
    "        \n",
    "You have below the **tf-idf values** for a few words. In our example, the word <i>gås</i> has the value 0 in bannlyst.txt and the value 0.000101001964 in nils.txt\n",
    "\n",
    "<pre>\n",
    "troll.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 2.148161748868631e-06\n",
    "\tet\t 0.0\n",
    "kejsaren.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 8.08284798629935e-06\n",
    "\tet\t 8.273225429362848e-05\n",
    "marbacka.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 7.582276564686669e-06\n",
    "\tet\t 9.70107989686256e-06\n",
    "herrgard.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 0.0\n",
    "\tet\t 0.0\n",
    "nils.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.00010100196417506702\n",
    "\tnils\t 0.00010164426900380124\n",
    "\tet\t 0.0\n",
    "osynliga.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 0.0\n",
    "\tet\t 0.0\n",
    "jerusalem.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 4.968292117670952e-06\n",
    "\tet\t 0.0\n",
    "bannlyst.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 0.0\n",
    "\tet\t 0.0\n",
    "gosta.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 0.0\n",
    "\tet\t 0.0\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, the tf-idf representation is a **vector**. In your program, you will keep this idea and use all the words in the corpus as keys: *Each dictionary will include all the words of the corpus as keys*. The value of the key is then possibly 0, meaning that the word is not in the document or is in all the documents as for the word `nils` in `gosta.tx`. \n",
    "\n",
    "As further work, you may think of optimizing this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bannlyst.txt': 76079,\n",
       " 'gosta.txt': 129143,\n",
       " 'herrgard.txt': 33665,\n",
       " 'jerusalem.txt': 154141,\n",
       " 'kejsaren.txt': 63164,\n",
       " 'marbacka.txt': 67334,\n",
       " 'nils.txt': 198403,\n",
       " 'osynliga.txt': 82723,\n",
       " 'troll.txt': 118833}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dicts = {}\n",
    "word_counts = {}\n",
    "\n",
    "for wordkey in master_index:\n",
    "    #print(wordkey)\n",
    "    #word_dicts.update(dict(map(lambda d: (d[0], len(d[1])), master_index[wordkey].items())))\n",
    "    word_dicts = dict(map(lambda d: (d[0], len(d[1])), master_index[wordkey].items()))\n",
    "    for doc in word_dicts:\n",
    "        if not doc in word_counts:\n",
    "            word_counts[doc] = word_dicts[doc]\n",
    "        else:\n",
    "            word_counts[doc] += word_dicts[doc]\n",
    "    #print(word_dicts)\n",
    "#word_dicts\n",
    "word_counts\n",
    "#master_index['glömden']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# tf-idf representation as a dictionary/vector\n",
    "tfidf_dict = {}\n",
    "\n",
    "for wordkey in master_index:\n",
    "    #print(wordkey)\n",
    "    for file in master_index[wordkey]:\n",
    "        # list of tokens (words): match objects\n",
    "        #filewords = tokenize(filetext.lower().strip())\n",
    "        \n",
    "        #word_dicts = dict(map(lambda d: (d[0], len(d[1])), master_index[wordkey].items()))\n",
    "        \n",
    "        \n",
    "        filesize = word_counts[file]\n",
    "        #print(file, \":\\t\",filesize)\n",
    "        \n",
    "        # Term frequency\n",
    "        tf = len(master_index[wordkey][file]) / filesize\n",
    "        #print(\"tf: \", tf)\n",
    "        \n",
    "        # calculate idf value\n",
    "        # N is the total number of documents in the corpus\n",
    "        N = len(corpus_files)\n",
    "        # nj is the number of documents where term j occurs\n",
    "        nj = len(master_index[wordkey])\n",
    "        idf = math.log10(N / nj)\n",
    "        #print(\"idf: \", idf)\n",
    "        \n",
    "        tfxidf = tf * idf\n",
    "        if file in tfidf_dict:\n",
    "            tfidf_dict[file][wordkey] = tfxidf\n",
    "        else:\n",
    "            tfidf_dict[file] = {}\n",
    "            tfidf_dict[file][wordkey] = 0\n",
    "            \n",
    "tfidf = tfidf_dict\n",
    "#tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf['troll.txt']['känna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1481617488686316e-06"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf['troll.txt']['nils']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Documents <a name=\"t6\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the **cosine similarity**, compare all the pairs of documents with their tf-idf representation and present your results in a table. You will include this table in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function computing the cosine similarity between two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def cos_sim(doc1, doc2):\n",
    "    # The intersection() method returns a set that \n",
    "    # contains the similarity between two or more sets.\n",
    "    sim_set = set(doc1.keys()).intersection(set(doc2.keys()))\n",
    "    \n",
    "    # implement the formula of cosine similarity \n",
    "    # numerator values\n",
    "    num = sum(doc1[key] * doc2[key] for key in sim_set)\n",
    "    \n",
    "    # denominator values\n",
    "    norm_doc1 = math.sqrt(sum(doc1[w]*doc1[w] for w in doc1.keys()))\n",
    "    norm_doc2 = math.sqrt(sum(doc2[w]*doc2[w] for w in doc2.keys()))\n",
    "    denom = norm_doc1 * norm_doc2\n",
    "    return num / denom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute the similarity matrix between the documents of the corpus. While computing the similarities, you will record the two most similar documents that you will call `most_sim_doc1` and `most_sim_doc2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbannlyst.txt gosta.txt herrgard.txt jerusalem.txt kejsaren.txt marbacka.txt nils.txt osynliga.txt troll.txt \n",
      "bannlyst.txt\t1.0\t0.049\t0.0009\t0.0065\t0.024\t0.0368\t0.051\t0.0521\t0.0886\n",
      "gosta.txt\t0.049\t1.0\t0.0031\t0.0043\t0.048\t0.0802\t0.1048\t0.1248\t0.1957\n",
      "herrgard.txt\t0.0009\t0.0031\t1.0\t0.3707\t0.0007\t0.0036\t0.0051\t0.0048\t0.0041\n",
      "jerusalem.txt\t0.0065\t0.0043\t0.3707\t1.0\t0.0018\t0.0049\t0.0045\t0.0283\t0.0071\n",
      "kejsaren.txt\t0.024\t0.048\t0.0007\t0.0018\t1.0\t0.0711\t0.0497\t0.0511\t0.1813\n",
      "marbacka.txt\t0.0368\t0.0802\t0.0036\t0.0049\t0.0711\t1.0\t0.0847\t0.0932\t0.1472\n",
      "nils.txt\t0.051\t0.1048\t0.0051\t0.0045\t0.0497\t0.0847\t1.0\t0.1106\t0.1885\n",
      "osynliga.txt\t0.0521\t0.1248\t0.0048\t0.0283\t0.0511\t0.0932\t0.1106\t1.0\t0.1926\n",
      "troll.txt\t0.0886\t0.1957\t0.0041\t0.0071\t0.1813\t0.1472\t0.1885\t0.1926\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "sim_matrix = {}\n",
    "cols = \"\\t\"\n",
    "for file in corpus_files:\n",
    "    cols += file + \" \"\n",
    "print(cols)\n",
    "\n",
    "for f1 in corpus_files:\n",
    "    rows = f1\n",
    "    for f2 in corpus_files:\n",
    "        if not f1 in sim_matrix:\n",
    "            sim_matrix[f1] = {}\n",
    "        sim_matrix[f1][f2] = cos_sim(tfidf[f1], tfidf[f2])\n",
    "        rows += \"\\t\" + str(round(cos_sim(tfidf[f1], tfidf[f2]), 4))\n",
    "        \n",
    "    print(rows)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the similarity matrix - second method using **array of 2 dimensions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sim_list = []\n",
    "for i in range(len(corpus_files)):\n",
    "#for i in corpus_files:\n",
    "    sim_list = sim_list + [[]]\n",
    "    for j in range(len(corpus_files)):\n",
    "    #for j in corpus_files:\n",
    "        sim_list[i] = sim_list[i] + [0]\n",
    "        f1 = corpus_files[i]\n",
    "        #print(\"f1: \", f1)\n",
    "        f2 = corpus_files[j]\n",
    "        #print(\"f2: \", f2)\n",
    "        #simList[i][j] = cos_sim(tfidf[corpus_files[i]],tfidf[corpus_files[j]])\n",
    "        sim_list[i][j] = cos_sim(tfidf[f1],tfidf[f2])\n",
    "        #sim_list[i][j] = round(cos_sim(tfidf[f1], tfidf[f2]), 4)\n",
    "        #print(sim_list[i][j])\n",
    "#print(sim_list)\n",
    "sim_matrix = numpy.matrix(sim_list)\n",
    "\n",
    "#print (sim_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Give the name of the two novels that are the most similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diag_filtered = filter(lambda f: f[0] != f[1], ((i,j) for i, j in range(len(corpus_files))) )\n",
    "#word_dicts = dict(map(lambda d: (d[0], len(d[1])), master_index[wordkey].items()))\n",
    "diag_filtered = {}\n",
    "max_sim = {}\n",
    "for file in sim_matrix:\n",
    "    #print(list(sim_matrix[file].values()))\n",
    "    #print(list(sim_matrix[file].keys()))\n",
    "    #fileXtfidf_dict = dict(sim_matrix[file].items())\n",
    "    \n",
    "    # filter away the diagonal elements\n",
    "    diag_filtered[file] = dict(filter(lambda f: file != f[0], sim_matrix[file].items() ))\n",
    "    \n",
    "    fileXtfidf_dict = dict(diag_filtered[file].items())\n",
    "    maxvalue = max(map(lambda f: (f[1]), diag_filtered[file].items()))\n",
    "    #max_sim[file] = max(map(lambda f: (f[1]), diag_filtered[file].items()))\n",
    "    \n",
    "    for key in list(fileXtfidf_dict.keys()):\n",
    "        if fileXtfidf_dict.get(key) == maxvalue:\n",
    "            max_sim[file] = {key : maxvalue}\n",
    "    #print(max_sim[file])\n",
    "    #max_sim[file] = max(filter(lambda f: (f[1]), diag_filtered[file].items()))\n",
    "    #X = fileXtfidf_dict.get(max_sim[file])\n",
    "    #print(X)\n",
    "#print(\" Similarity list \\n\", max_sim)\n",
    "#max_sim.values()\n",
    "#for k in list(sim_matrix[file].keys()):\n",
    "#similar = max(map(lambda f: (f[1]), max_sim.values()))\n",
    "#print(similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_values(nested_dict, allvalues, keykeys):\n",
    "        \n",
    "    for keykey, value in nested_dict.items():\n",
    "    #for value in nested_dict.items():\n",
    "        if type(value) is dict:\n",
    "            #allvalues = []\n",
    "            get_all_values(value, allvalues, keykeys)\n",
    "            #allvalues = []\n",
    "        else:\n",
    "            #print(keykey, \":\", value)\n",
    "            keykeys.append(keykey)\n",
    "            allvalues.append(value)\n",
    "    #print(\"all values: \", allvalues)\n",
    "    \n",
    "    most = max(allvalues)\n",
    "    idx = allvalues.index(most)\n",
    "    return keykeys, allvalues, most, idx       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(max_sim.keys())\n",
    "#v = list(max_sim.values())\n",
    "massimi = []\n",
    "innerKeys = []\n",
    "innerDocs, maxlist, most, idx = get_all_values(max_sim, massimi, innerKeys)\n",
    "#print(\"all keys: \", innerDocs)\n",
    "most_sim_doc1 = k[idx]\n",
    "most_sim_doc2 = innerDocs[idx]\n",
    "max_similarity = most\n",
    "\n",
    "#innerDict = v[idx]\n",
    "#print(innerDict)\n",
    "#allvalues = get_all_values(max_sim)\n",
    "#print(\"\\n file1\", most_sim_doc1)\n",
    "#print(\"\\n file2\", most_sim_doc2)\n",
    "#print(most, \" at index: \", idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar: herrgard.txt jerusalem.txt Similarity: 0.370689423873392\n"
     ]
    }
   ],
   "source": [
    "print(\"Most similar:\", most_sim_doc1, most_sim_doc2, \"Similarity:\", max_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission <a name=\"t7\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have written all the code and run all the cells, fill in your ID and as well as the name of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "STIL_ID = [\"hi8826mo-s\"] # Write your stil ids\n",
    "CURRENT_NOTEBOOK_PATH = os.path.join(os.getcwd(), \n",
    "                                     \"1-indexer_solution_HichamMohamad.ipynb\") # Write the name of your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission code will send your answer. It consists of the two most similar novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'herrgard.txt jerusalem.txt'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANSWER = ' '.join(sorted([most_sim_doc1, most_sim_doc2]))\n",
    "ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the moment of truth:\n",
    "1. Save your notebook and\n",
    "2. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_NOTEBOOK_PATH = CURRENT_NOTEBOOK_PATH + \".submission.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSIGNMENT = 1\n",
    "API_KEY = \"f581ba347babfea0b8f2c74a3a6776a7\"\n",
    "\n",
    "# Copy and compress current notebook\n",
    "with bz2.open(SUBMISSION_NOTEBOOK_PATH, mode=\"wb\") as fout:\n",
    "    with open(CURRENT_NOTEBOOK_PATH, \"rb\") as fin:\n",
    "        fout.write(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hashu\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'msg': None,\n",
       " 'status': 'correct',\n",
       " 'signature': '62721125e82d86bd3aa72c8dd7badde1f74e81f298a90850c7e0a840db1e737c4408d3e80577074bf38362124265d5350d5fac0b09e644061c4a09a5abe34963',\n",
       " 'submission_id': 'f8a956e1-37c2-42b3-8e82-cf7d62cf9838'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.post(\"https://vilde.cs.lth.se/edan20checker/submit\", \n",
    "                    files={\"notebook_file\": open(SUBMISSION_NOTEBOOK_PATH, \"rb\")}, \n",
    "                    data={\n",
    "                        \"stil_id\": STIL_ID,\n",
    "                        \"assignment\": ASSIGNMENT,\n",
    "                        \"answer\": ANSWER,\n",
    "                        \"api_key\": API_KEY,\n",
    "                    },\n",
    "                   verify=False)\n",
    "\n",
    "\n",
    "# from IPython.display import display, JSON\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `status` and be sure it is `correct`. If not, revise your code; verify that you obtained intermediate results identical to those in the notebook; and resubmit your notebook. You can submit multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Reading</h2> <a name=\"t8\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are done, it is time to write your **individual report**. You will describe the indexer and comment the results.\n",
    "\n",
    "You will also read the text: <i>Challenges in Building Large-Scale Information Retrieval Systems</i> about the history of <a href=\"https://research.google.com/people/jeff/WSDM09-keynote.pdf\">Google indexing</a> by <a href=\"https://research.google.com/pubs/jeff.html\">Jeff Dean</a>.\n",
    "\n",
    "In your report, you will tell how your index encoding is related to what **Google** did. You must identify the slide where you have the most similar indexing technique and write the slide number in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
